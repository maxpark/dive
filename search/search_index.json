{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"DIVE Documentation This is the user documentation for DIVE, an open-source annotation and analysis platform which can be run either as a web or desktop application. DIVE was originally built for, and has deep integration with, the VIAME toolkit , but it can also be integrated with other automated analytics. Try the web version Get the desktop app Get Help For feedback, problems, questions, or feature requests, please email viame-web@kitware.com . Our team would be happy to hear from you! Features Current capabilities of DIVE include: User import of frame images or video. Playback of existing annotation data. Manual creation of new annotations. Automatic object detection and tracking of user-imported data. Manual user refinement of automatically generated tracks Export of generated annotations. Definitions DIVE is the annotator and data management software system. It is our name for the code and capabilities, including both web and desktop, that can be deployed and configured for a variety of needs. VIAME (Video and Image Analytics for Marine Environments) is a suite of computer vision tools for object detection, tracking, rapid model generation, and many other types of analysis. Get more info at viametoolkit.org VIAME Web is the specific DIVE Web deployment at viame.kitware.com . It includes a web-based annotator with the capabilities to run VIAME workflows on user-provided data. You may deploy the web system into your own labratory or cloud environment.","title":"Home"},{"location":"#dive-documentation","text":"This is the user documentation for DIVE, an open-source annotation and analysis platform which can be run either as a web or desktop application. DIVE was originally built for, and has deep integration with, the VIAME toolkit , but it can also be integrated with other automated analytics. Try the web version Get the desktop app","title":"DIVE Documentation"},{"location":"#get-help","text":"For feedback, problems, questions, or feature requests, please email viame-web@kitware.com . Our team would be happy to hear from you!","title":"Get Help"},{"location":"#features","text":"Current capabilities of DIVE include: User import of frame images or video. Playback of existing annotation data. Manual creation of new annotations. Automatic object detection and tracking of user-imported data. Manual user refinement of automatically generated tracks Export of generated annotations.","title":"Features"},{"location":"#definitions","text":"DIVE is the annotator and data management software system. It is our name for the code and capabilities, including both web and desktop, that can be deployed and configured for a variety of needs. VIAME (Video and Image Analytics for Marine Environments) is a suite of computer vision tools for object detection, tracking, rapid model generation, and many other types of analysis. Get more info at viametoolkit.org VIAME Web is the specific DIVE Web deployment at viame.kitware.com . It includes a web-based annotator with the capabilities to run VIAME workflows on user-provided data. You may deploy the web system into your own labratory or cloud environment.","title":"Definitions"},{"location":"Annotation-QuickStart/","text":"Annotation Quickstart Single Frame Detections - creating multiple detections on a single image frame quickly Track Annotations - How to quickly create track annotations for a video or image sequence Interpolation Mode - fastest and easiest way to generate track annotations Advance Frame Mode - This mode keeps you editing the same track while automatically advancing the frame each time a detection is drawn. In most cases the interpolation mode will be preferred. Polygon Annotations Head/Tail Annotations Important Keyboard Shortcuts Single Frame Detections Click the Settings wheel next to the '+Track' Icon From the dropdown choose \u201cDetection\u201d mode. Choose a type from the dropdown or type a name that you want all detections to inherit. If the type doesn't exist just type the name to create a new one. You should see the \"+Detection\" button change color to match the type. Now if you hover over the button it will display the default type. Turn on continuous mode if you would like to keep dragging the mouse to create detections. Create your first detection by clicking and dragging to generate a rectangle. If you are in continuous mode just click and drag again to create the next detection Otherwise edit the detection and then hit the \u201cN\u201d key on the keyboard or click on detection to create another detection. Single Detection Mode Demo The Demo below shows how to use Detection mode to quickly create numerous detections of the same type. Track Annotations Interpolation Mode Next to the \"+Track\" button click the Settings Icon to open the Creation Settings From the dropdown make sure that \u201cTrack\u201d mode is selected. Also ensure that interpolation is turned on. Either click \u201c+Track\u201d or hit \u201cN\u201d on the keyboard to create the initial track. You can now advance through frames and an outline of the previous annotation will remain. You can advance either by using Left Arrow/Right Arrow or clicking on the timeline to a specific position. To set another frame you only need to move or resize the transparent annotation. There are controls on for the currently selected track to add/remove keyframes. The \u201cStar\u201d icon will allow you to add and remove keyframes. The \"interpolate icon\" (two overlapped dashed rectangles) will turn on/off interpolation for the current region you are in. During editing there is a view within the event viewer which allows you to see where interpolation occurs and where the keyframes are located. If a line is not connecting two keyframes in a track then interpolation is disabled for that section. Interpolation Mode Demo The Demo below shows how to use Interpolation mode to quickly crate tracks for objects through a video/image-sequence. Advance Frame Next to the \"+Track\" button click the Settings Icon to open the Creation Settings From the dropdown make sure that \u201cTrack\u201d mode is selected. Turn off interpolation and turn on \"AdvanceFrame\" Now each time an individual detection is drawn the frame will automatically advance to the next frame. Advance Frame Mode Demo The Demo below shows how to use AdvanceFrame mode to travel through the video while creating annotations. Head Tail Annotations Head/Tail creation on an existing Track: Either create a new detection/track or select an existing one with \u201cLeft Click\u201d When a track is selected Head/Tail annotations can be added by either going to the edit menu and selecting the line tool at the top of screen or by: H Key - Create Head annotation T Key - Create Tail annotation After entering head/tail annotation mode the cursor becomes a crosshair and the the head or tail can be placed. Once the first marker is placed it automatically transitions to the second marker. (If you start with head, the second one will be the tail and vice versa if you start with tail) Creating New Track from Head/Tail Annotation: You can create a track by starting with a head/tail annotation or just a single point. After clicking \"+Track\" you can switch the editing mode into the line tool. This will allow you place a single point initially by clicking You can then either hit escape and a bounding box will appear around that point Or you can place the second point and a new bounding box will be created which can be edited. Notes: The head point is denoted by a filled circle, while the tail point is denoted by a hollow circle. You can also start with the tail: tap t - Indicates you are in tail mode and are going to place the tail point. You don't have to place both markers. Press Escape on your keyboard at anytime to exit out of the line creation mode.. Your cursor will change back to a pointer. You can modify an existing head/tail marker by placing the annotation into 'Edit Mode' and then selecting the line tool from the editing options. To Delete a head/tail pair , select a detection with existing markers and have it in line edit mode. This will allow you click the Delete button at the top to delete both points. Fish Head Tail Demo Polygon Annotations While every track is required to have a bounding box a polygon can be added as well for more detailed annotations of objects. When a polygon is created or edited it will adjust the bounding box to fit the size of the polygon. Polygon Creation Click on \"+Track\" or hit the 'N' key then select the polygon tool - Start by placing the first point and then start placing additional points for the polygon NOTE: You can hold down the mouse to draw points quickly You can either close the polygon manually or \"Right-Click\" to automatically close the polygon Polygon Editing In edit mode for a polygon you can select any large circle to move it. This will move the point to a new position and recalculate the bounding box. The smaller circles allow you you to create new points which can be used to adjust the polygon and make it appear smoother. A single click on a point will select a point which can then be deleted by using the delete button Polygon Demo","title":"Annotation Quickstart"},{"location":"Annotation-QuickStart/#annotation-quickstart","text":"Single Frame Detections - creating multiple detections on a single image frame quickly Track Annotations - How to quickly create track annotations for a video or image sequence Interpolation Mode - fastest and easiest way to generate track annotations Advance Frame Mode - This mode keeps you editing the same track while automatically advancing the frame each time a detection is drawn. In most cases the interpolation mode will be preferred. Polygon Annotations Head/Tail Annotations Important Keyboard Shortcuts","title":"Annotation Quickstart"},{"location":"Annotation-QuickStart/#single-frame-detections","text":"Click the Settings wheel next to the '+Track' Icon From the dropdown choose \u201cDetection\u201d mode. Choose a type from the dropdown or type a name that you want all detections to inherit. If the type doesn't exist just type the name to create a new one. You should see the \"+Detection\" button change color to match the type. Now if you hover over the button it will display the default type. Turn on continuous mode if you would like to keep dragging the mouse to create detections. Create your first detection by clicking and dragging to generate a rectangle. If you are in continuous mode just click and drag again to create the next detection Otherwise edit the detection and then hit the \u201cN\u201d key on the keyboard or click on detection to create another detection.","title":"Single Frame Detections"},{"location":"Annotation-QuickStart/#single-detection-mode-demo","text":"The Demo below shows how to use Detection mode to quickly create numerous detections of the same type.","title":"Single Detection Mode Demo"},{"location":"Annotation-QuickStart/#track-annotations","text":"","title":"Track Annotations"},{"location":"Annotation-QuickStart/#interpolation-mode","text":"Next to the \"+Track\" button click the Settings Icon to open the Creation Settings From the dropdown make sure that \u201cTrack\u201d mode is selected. Also ensure that interpolation is turned on. Either click \u201c+Track\u201d or hit \u201cN\u201d on the keyboard to create the initial track. You can now advance through frames and an outline of the previous annotation will remain. You can advance either by using Left Arrow/Right Arrow or clicking on the timeline to a specific position. To set another frame you only need to move or resize the transparent annotation. There are controls on for the currently selected track to add/remove keyframes. The \u201cStar\u201d icon will allow you to add and remove keyframes. The \"interpolate icon\" (two overlapped dashed rectangles) will turn on/off interpolation for the current region you are in. During editing there is a view within the event viewer which allows you to see where interpolation occurs and where the keyframes are located. If a line is not connecting two keyframes in a track then interpolation is disabled for that section.","title":"Interpolation Mode"},{"location":"Annotation-QuickStart/#interpolation-mode-demo","text":"The Demo below shows how to use Interpolation mode to quickly crate tracks for objects through a video/image-sequence.","title":"Interpolation Mode Demo"},{"location":"Annotation-QuickStart/#advance-frame","text":"Next to the \"+Track\" button click the Settings Icon to open the Creation Settings From the dropdown make sure that \u201cTrack\u201d mode is selected. Turn off interpolation and turn on \"AdvanceFrame\" Now each time an individual detection is drawn the frame will automatically advance to the next frame.","title":"Advance Frame"},{"location":"Annotation-QuickStart/#advance-frame-mode-demo","text":"The Demo below shows how to use AdvanceFrame mode to travel through the video while creating annotations.","title":"Advance Frame Mode Demo"},{"location":"Annotation-QuickStart/#head-tail-annotations","text":"Head/Tail creation on an existing Track: Either create a new detection/track or select an existing one with \u201cLeft Click\u201d When a track is selected Head/Tail annotations can be added by either going to the edit menu and selecting the line tool at the top of screen or by: H Key - Create Head annotation T Key - Create Tail annotation After entering head/tail annotation mode the cursor becomes a crosshair and the the head or tail can be placed. Once the first marker is placed it automatically transitions to the second marker. (If you start with head, the second one will be the tail and vice versa if you start with tail) Creating New Track from Head/Tail Annotation: You can create a track by starting with a head/tail annotation or just a single point. After clicking \"+Track\" you can switch the editing mode into the line tool. This will allow you place a single point initially by clicking You can then either hit escape and a bounding box will appear around that point Or you can place the second point and a new bounding box will be created which can be edited. Notes: The head point is denoted by a filled circle, while the tail point is denoted by a hollow circle. You can also start with the tail: tap t - Indicates you are in tail mode and are going to place the tail point. You don't have to place both markers. Press Escape on your keyboard at anytime to exit out of the line creation mode.. Your cursor will change back to a pointer. You can modify an existing head/tail marker by placing the annotation into 'Edit Mode' and then selecting the line tool from the editing options. To Delete a head/tail pair , select a detection with existing markers and have it in line edit mode. This will allow you click the Delete button at the top to delete both points.","title":"Head Tail Annotations"},{"location":"Annotation-QuickStart/#fish-head-tail-demo","text":"","title":"Fish Head Tail Demo"},{"location":"Annotation-QuickStart/#polygon-annotations","text":"While every track is required to have a bounding box a polygon can be added as well for more detailed annotations of objects. When a polygon is created or edited it will adjust the bounding box to fit the size of the polygon.","title":"Polygon Annotations"},{"location":"Annotation-QuickStart/#polygon-creation","text":"Click on \"+Track\" or hit the 'N' key then select the polygon tool - Start by placing the first point and then start placing additional points for the polygon NOTE: You can hold down the mouse to draw points quickly You can either close the polygon manually or \"Right-Click\" to automatically close the polygon","title":"Polygon Creation"},{"location":"Annotation-QuickStart/#polygon-editing","text":"In edit mode for a polygon you can select any large circle to move it. This will move the point to a new position and recalculate the bounding box. The smaller circles allow you you to create new points which can be used to adjust the polygon and make it appear smoother. A single click on a point will select a point which can then be deleted by using the delete button","title":"Polygon Editing"},{"location":"Annotation-QuickStart/#polygon-demo","text":"","title":"Polygon Demo"},{"location":"Annotation-User-Interface-Overview/","text":"Annotation User Interface Overview The User Interface for the Annotation Editor is broken into a few main areas: Navigation Bar - Controls to return back to browser as well as perform higher level functions such as running pipelines. Also the ability to save annotations to the server. Edit Bar - Controls the viewing of annotations on screen and allows for the editing/creation of annotations. Annotation View - where the image/video is displayed as well as all annotations Type List - A list of all the types of tracks/detections on the page that can be used to filter the current view. Track List - List of all the tracks as well as providing a way to perform editing functions on those tracks. Timeline - timeline view of tracks and detections, as well as an interface to control the current frame along the video/image-sequence Attributes - Attributes panel used to assign attributes to individual tracks or detections. Concepts and Definitions Detection - A single slice of a track at a point in time. Unusually the point of time is a frame of a video or a sequence of images. Features - Bounding Rectangle, Head/Tail or other visible elements of a detection. Track - A collection of detections spanned over multiple frames in a video or image sequence. Tracks include a start and end time and can have periods in which no detections exist. Types - a set of tracks that share specific display properties including color, line thickness, opacity. Frame - a single image or point in time for a video or image sequence KeyFrame - While using interpolation for tracks these are indicated as a locked position. Interpolation is calculated linearly between keyframes to draw the bounding rects. Interpolation - linearly moving the bounding rect of a detection over time between two keyframes.","title":"Introduction"},{"location":"Annotation-User-Interface-Overview/#annotation-user-interface-overview","text":"The User Interface for the Annotation Editor is broken into a few main areas: Navigation Bar - Controls to return back to browser as well as perform higher level functions such as running pipelines. Also the ability to save annotations to the server. Edit Bar - Controls the viewing of annotations on screen and allows for the editing/creation of annotations. Annotation View - where the image/video is displayed as well as all annotations Type List - A list of all the types of tracks/detections on the page that can be used to filter the current view. Track List - List of all the tracks as well as providing a way to perform editing functions on those tracks. Timeline - timeline view of tracks and detections, as well as an interface to control the current frame along the video/image-sequence Attributes - Attributes panel used to assign attributes to individual tracks or detections.","title":"Annotation User Interface Overview"},{"location":"Annotation-User-Interface-Overview/#concepts-and-definitions","text":"Detection - A single slice of a track at a point in time. Unusually the point of time is a frame of a video or a sequence of images. Features - Bounding Rectangle, Head/Tail or other visible elements of a detection. Track - A collection of detections spanned over multiple frames in a video or image sequence. Tracks include a start and end time and can have periods in which no detections exist. Types - a set of tracks that share specific display properties including color, line thickness, opacity. Frame - a single image or point in time for a video or image sequence KeyFrame - While using interpolation for tracks these are indicated as a locked position. Interpolation is calculated linearly between keyframes to draw the bounding rects. Interpolation - linearly moving the bounding rect of a detection over time between two keyframes.","title":"Concepts and Definitions"},{"location":"Attributes/","text":"Attributes Attributes are properties that can be assigned to an entire track or a single detection within a track. Attributes are created within a dataset using the Track Details panel. Background/Terms It\u2019s important to note that there are attribute definitations and then track/detection attributes. Attributes definitions are all the possible attributes that can be assigned to tracks or detection. Think of them as a template for the attributes that can be set on tracks/detections. Track attributes are information for the entire track and detection attributes are information for the specific frame or instance of time. Attribute Definition - base attribute which defines the name and type for either a track or detection attribute Track Attribute - attribute with a value associated with the entire track Detection Attribute - attribute with a value specified for a detection during a single frame Example: 1 2 3 4 5 6 7 - Attribute Definitions - Track - CompleteTrack: Boolean - FishLength: number (cm) - Detection - Swimming: Boolean - Eating: Boolean Usage of the above Attribute Definitions for a track and its detections: 1 2 3 4 5 6 7 8 9 - Fish Track 1 - Track Attributes - FishLength: 20 - Detection Attributes - Frame 1 - Eating: true - Frame 2 - Swimming: false - Eating: false Note: All Attribute definitions do not need to be assigned to values. CompleteTrack (Track Attribute) and Swimming for Frame 1 (Detection Attribute) weren't assigned in this example. Navigating to Attributes Controls for defining and editing attributes are found on the Track Details Panel. While your data is open you can select a track/detection to apply attributes. Open the Track Details page by clicking on the eye icon or by using the \u2018A\u2019 key as a shortcut. Here you will see the track/detection type, confidence pairs associated with it and then a list of track and detection attributes. For attributes there are two sections Track Attributes - All track level attributes Detection Attributes - attributes associated with the track on a per frame basis Info During import of a VIAME CSV file, attributes that are structured using the VIAME CSV specification will automatically show up in the list. The type of the attribute is guessed by examining values and may need to be manually corrected. Adding Attribute Definitions By default the view on the page shows all attributes associated with the dataset in editing mode. If you click on the Eye Icon this will only display the attributes that are set on the selected track and the current frame for that track. After clicking the eye icon, all attributes that are unset will be removed from view. This is useful for reviewing or viewing attributes of a track when there is a large number of attribute definitions. To add attributes click on the corresponding \"+Attribute\" icon for either a track or detection attribute This will bring up a New Attribute dialog where you can enter a unique name for the attribute Next choose a datatype: Number Boolean - (True/False) Text Custom text that the user provides A predefined list of text that can be chosen from After choosing a Track/Detection and a Datatype click Save to add the new attribute Editing Dataset Attributes Click on the setting icon next to an existing attribute to edit it's definition details Note: Editing or deleting an attribute definition doesn\u2019t change the existing track/detection attributes for the data Deleting an attribute definition will cause it to disappear from the list. A future update will cause the attribute to also be removed from all tracks/detections. Editing an attribute definition will change the way the controls behave, but will not change any existing set values. Setting Track/Detection Attribute Values Click on the attribute value when in viewing mode to edit and set the attribute Or directly edit the value field when in the attribute editing mode Setting an attribute to the null/empty value will remove the value from the track/detection Importing and Exporting Attributes Attributes are part of the dataset configuration that can be imported and exported. Set up a dataset with all the attributes you need In the download menu, create a configuration export Use this configuration with other datasets Use the import button to load this configuration to other datasets. Upload the configuration file when you create new datasets to initialize them with an existing configuration. Applying Attributes Demo","title":"Attributes"},{"location":"Attributes/#attributes","text":"Attributes are properties that can be assigned to an entire track or a single detection within a track. Attributes are created within a dataset using the Track Details panel.","title":"Attributes"},{"location":"Attributes/#backgroundterms","text":"It\u2019s important to note that there are attribute definitations and then track/detection attributes. Attributes definitions are all the possible attributes that can be assigned to tracks or detection. Think of them as a template for the attributes that can be set on tracks/detections. Track attributes are information for the entire track and detection attributes are information for the specific frame or instance of time. Attribute Definition - base attribute which defines the name and type for either a track or detection attribute Track Attribute - attribute with a value associated with the entire track Detection Attribute - attribute with a value specified for a detection during a single frame Example: 1 2 3 4 5 6 7 - Attribute Definitions - Track - CompleteTrack: Boolean - FishLength: number (cm) - Detection - Swimming: Boolean - Eating: Boolean Usage of the above Attribute Definitions for a track and its detections: 1 2 3 4 5 6 7 8 9 - Fish Track 1 - Track Attributes - FishLength: 20 - Detection Attributes - Frame 1 - Eating: true - Frame 2 - Swimming: false - Eating: false Note: All Attribute definitions do not need to be assigned to values. CompleteTrack (Track Attribute) and Swimming for Frame 1 (Detection Attribute) weren't assigned in this example.","title":"Background/Terms"},{"location":"Attributes/#navigating-to-attributes","text":"Controls for defining and editing attributes are found on the Track Details Panel. While your data is open you can select a track/detection to apply attributes. Open the Track Details page by clicking on the eye icon or by using the \u2018A\u2019 key as a shortcut. Here you will see the track/detection type, confidence pairs associated with it and then a list of track and detection attributes. For attributes there are two sections Track Attributes - All track level attributes Detection Attributes - attributes associated with the track on a per frame basis Info During import of a VIAME CSV file, attributes that are structured using the VIAME CSV specification will automatically show up in the list. The type of the attribute is guessed by examining values and may need to be manually corrected.","title":"Navigating to Attributes"},{"location":"Attributes/#adding-attribute-definitions","text":"By default the view on the page shows all attributes associated with the dataset in editing mode. If you click on the Eye Icon this will only display the attributes that are set on the selected track and the current frame for that track. After clicking the eye icon, all attributes that are unset will be removed from view. This is useful for reviewing or viewing attributes of a track when there is a large number of attribute definitions. To add attributes click on the corresponding \"+Attribute\" icon for either a track or detection attribute This will bring up a New Attribute dialog where you can enter a unique name for the attribute Next choose a datatype: Number Boolean - (True/False) Text Custom text that the user provides A predefined list of text that can be chosen from After choosing a Track/Detection and a Datatype click Save to add the new attribute","title":"Adding Attribute Definitions"},{"location":"Attributes/#editing-dataset-attributes","text":"Click on the setting icon next to an existing attribute to edit it's definition details Note: Editing or deleting an attribute definition doesn\u2019t change the existing track/detection attributes for the data Deleting an attribute definition will cause it to disappear from the list. A future update will cause the attribute to also be removed from all tracks/detections. Editing an attribute definition will change the way the controls behave, but will not change any existing set values.","title":"Editing Dataset Attributes"},{"location":"Attributes/#setting-trackdetection-attribute-values","text":"Click on the attribute value when in viewing mode to edit and set the attribute Or directly edit the value field when in the attribute editing mode Setting an attribute to the null/empty value will remove the value from the track/detection","title":"Setting Track/Detection Attribute Values"},{"location":"Attributes/#importing-and-exporting-attributes","text":"Attributes are part of the dataset configuration that can be imported and exported. Set up a dataset with all the attributes you need In the download menu, create a configuration export Use this configuration with other datasets Use the import button to load this configuration to other datasets. Upload the configuration file when you create new datasets to initialize them with an existing configuration.","title":"Importing and Exporting Attributes"},{"location":"Attributes/#applying-attributes-demo","text":"","title":"Applying Attributes Demo"},{"location":"Command-Line-Tools/","text":"DIVE Command Line Tools Note This page is not related to the VIAME command line (i.e. kwiver , viame_train_detector ) Some of the DIVE data conversion features are exposed through dive . Features Convert between VIAME CSV, DIVE Json, kpf, and coco. Verify the integrity of a DIVE Json annotation file. Installation 1 2 # Install the command line tools directly from source pip3 install git+https://github.com/Kitware/dive.git@main#subdirectory = server Usage 1 2 3 4 5 6 7 8 9 10 11 12 13 ~$ dive convert --help # Usage: dive convert [OPTIONS] COMMAND [ARGS]... # Options: # --version Show the version and exit. # --help Show this message and exit. # Commands: # coco2dive # dive2viame # kpf2dive # viame2dive","title":"Command Line Tools"},{"location":"Command-Line-Tools/#dive-command-line-tools","text":"Note This page is not related to the VIAME command line (i.e. kwiver , viame_train_detector ) Some of the DIVE data conversion features are exposed through dive .","title":"DIVE Command Line Tools"},{"location":"Command-Line-Tools/#features","text":"Convert between VIAME CSV, DIVE Json, kpf, and coco. Verify the integrity of a DIVE Json annotation file.","title":"Features"},{"location":"Command-Line-Tools/#installation","text":"1 2 # Install the command line tools directly from source pip3 install git+https://github.com/Kitware/dive.git@main#subdirectory = server","title":"Installation"},{"location":"Command-Line-Tools/#usage","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 ~$ dive convert --help # Usage: dive convert [OPTIONS] COMMAND [ARGS]... # Options: # --version Show the version and exit. # --help Show this message and exit. # Commands: # coco2dive # dive2viame # kpf2dive # viame2dive","title":"Usage"},{"location":"DataFormats/","text":"Data Formats DIVE Desktop and Web support a number of annotation and configuration formats. The following formats can be uploaded or imported alongside your media and will be automatically parsed. DIVE Annotation JSON (default annotation format) DIVE Configuration JSON VIAME CSV COCO and KWCOCO KPF (Kitware Packet Format) for MEVA DIVE Annotation JSON Files are typically named result_{dataset-name}.json . This JSON file is a map of numeric track identifiers to tracks, or Record<string, TrackData> , where TrackData is defined below. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 interface Feature { frame : number ; flick? : Readonly < number > ; interpolate? : boolean ; keyframe? : boolean ; bounds ?: [ number , number , number , number ]; // [x1, y1, x2, y2] as (left, top), (bottom, right) geometry? : GeoJSON.FeatureCollection < GeoJSON . Point | GeoJSON . Polygon | GeoJSON . LineString | GeoJSON . Point > ; fishLength? : number ; attributes? : Record < string , unknown > ; head ?: [ number , number ]; tail ?: [ number , number ]; } interface TrackData { trackId : TrackId ; meta : Record < string , unknown > ; attributes : Record < string , unknown > ; confidencePairs : Array < [ string , number ] > ; features : Array < Feature > ; begin : number ; end : number ; } The source TrackData definition can be found here as a TypeScript interface. Example DIVE annotation file This is a relatively simple example, and many optional fields are not included. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 { \"1\" : { \"trackId\" : 1 , \"meta\" : {}, \"attributes\" : {}, \"confidencePairs\" : [[ \"fish\" , 0.87 ], [ \"rock\" , 0.22 ]], \"features\" : [ { \"frame\" : 0 , \"bounds\" : [ 0 , 0 , 10 , 10 ], \"interpolate\" : true }, { \"frame\" : 2 , \"bounds\" : [ 10 , 10 , 20 , 20 ] }, ], \"begin\" : 0 , \"end\" : 2 , }, } DIVE Configuration JSON This information provides the specification for an individual dataset. It consists of the following. Allowed types (or labels) and their appearances are defined by customTypeStyling Preset confidence filters for those types are defined in confidenceFilters Track and Detection attribute specifications are defined in attributes The full DatasetMetaMutable definition can be found here . 1 2 3 4 5 6 interface DatasetMetaMutable { version : number ; customTypeStyling? : Record < string , CustomStyle > ; confidenceFilters? : Record < string , number > ; attributes? : Readonly < Record < string , Attribute >> ; } VIAME CSV Read the VIAME CSV Specification . COCO and KWCOCO Read the COCO Specification Read the KWCOCO Specification Kitware Packet Format (KPF) KPF does not have a public specification.","title":"Data Formats"},{"location":"DataFormats/#data-formats","text":"DIVE Desktop and Web support a number of annotation and configuration formats. The following formats can be uploaded or imported alongside your media and will be automatically parsed. DIVE Annotation JSON (default annotation format) DIVE Configuration JSON VIAME CSV COCO and KWCOCO KPF (Kitware Packet Format) for MEVA","title":"Data Formats"},{"location":"DataFormats/#dive-annotation-json","text":"Files are typically named result_{dataset-name}.json . This JSON file is a map of numeric track identifiers to tracks, or Record<string, TrackData> , where TrackData is defined below. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 interface Feature { frame : number ; flick? : Readonly < number > ; interpolate? : boolean ; keyframe? : boolean ; bounds ?: [ number , number , number , number ]; // [x1, y1, x2, y2] as (left, top), (bottom, right) geometry? : GeoJSON.FeatureCollection < GeoJSON . Point | GeoJSON . Polygon | GeoJSON . LineString | GeoJSON . Point > ; fishLength? : number ; attributes? : Record < string , unknown > ; head ?: [ number , number ]; tail ?: [ number , number ]; } interface TrackData { trackId : TrackId ; meta : Record < string , unknown > ; attributes : Record < string , unknown > ; confidencePairs : Array < [ string , number ] > ; features : Array < Feature > ; begin : number ; end : number ; } The source TrackData definition can be found here as a TypeScript interface. Example DIVE annotation file This is a relatively simple example, and many optional fields are not included. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 { \"1\" : { \"trackId\" : 1 , \"meta\" : {}, \"attributes\" : {}, \"confidencePairs\" : [[ \"fish\" , 0.87 ], [ \"rock\" , 0.22 ]], \"features\" : [ { \"frame\" : 0 , \"bounds\" : [ 0 , 0 , 10 , 10 ], \"interpolate\" : true }, { \"frame\" : 2 , \"bounds\" : [ 10 , 10 , 20 , 20 ] }, ], \"begin\" : 0 , \"end\" : 2 , }, }","title":"DIVE Annotation JSON"},{"location":"DataFormats/#dive-configuration-json","text":"This information provides the specification for an individual dataset. It consists of the following. Allowed types (or labels) and their appearances are defined by customTypeStyling Preset confidence filters for those types are defined in confidenceFilters Track and Detection attribute specifications are defined in attributes The full DatasetMetaMutable definition can be found here . 1 2 3 4 5 6 interface DatasetMetaMutable { version : number ; customTypeStyling? : Record < string , CustomStyle > ; confidenceFilters? : Record < string , number > ; attributes? : Readonly < Record < string , Attribute >> ; }","title":"DIVE Configuration JSON"},{"location":"DataFormats/#viame-csv","text":"Read the VIAME CSV Specification .","title":"VIAME CSV"},{"location":"DataFormats/#coco-and-kwcoco","text":"Read the COCO Specification Read the KWCOCO Specification","title":"COCO and KWCOCO"},{"location":"DataFormats/#kitware-packet-format-kpf","text":"KPF does not have a public specification.","title":"Kitware Packet Format (KPF)"},{"location":"Deployment-Docker-Compose/","text":"Running with Docker Compose Start here once you have SSH access and sudo privileges for a server or VM. Note Docker server installation is only supported on Linux distributions Container Images A DIVE Web deployment consists of 2 main services. kitware/viame-web - the web server kitware/viame-worker - the queue worker In addition, a database (MongoDB) and a queue service (RabbitMQ) are required. Install dependencies SSH into the target server and install these system dependencies. Tip You can skip this section if you used Ansible to configure your server, as it already installed all necessary dependencies. Install NVIDIA driver version >= 450.80.02 and CUDA 11.0+ sudo ubuntu-drivers install Install docker version 19.03+ guide Install docker-compose version 1.28.0+ guide Install nvidia-container-toolkit Basic deployment Clone this repository and configure options in .env . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Clone this repository git clone https://github.com/Kitware/dive /opt/dive # Change to correct directory cd /opt/dive # Initiate the .env file cp .env.default .env # Edit the .env file # See configuration options below and inline comments nano .env # Pull pre-built images docker-compose pull # Bring the services up docker-compose up -d VIAME server will be running at http://localhost:8010 . You should see a page that looks like this. The default username and password is admin:letmein . Production deployment If you have a server with a public-facing IP address and a domain name that points to it, you should be able to use our production deployment configuration. This is the way we deploy viame.kitware.com. containrrr/watchtower updates the running containers on a schedule using automated image builds from docker hub (above). linuxserver/duplicati is included to schedule nightly backups, but must be manually configured. You should scale the girder web server up to an appropriate number. This stack will automatically load-balance across however many instances you bring up. 1 2 3 4 5 6 7 8 # Continuing from above, modify .env again to include the production variables nano .env # pull extra containers docker-compose -f docker-compose.yml -f docker-compose.prod.yml pull # scale the web service up docker-compose -f docker-compose.yml -f docker-compose.prod.yml up -d --scale girder = 4 Addon management After initial deployment, a DIVE server will only have basic VIAME pipelines available. VIAME optional patches are installed and upgraded using a celery task that must be triggered by hand. Run this by issuing a POST /dive_configuration/upgrade_pipelines request from the swagger UI at http://{domain}/api/v1 . Whether you force or not, only those pipelines from addons from the exact urls passed will be enabled on the server. An old addon can be disabled by simply omitting its download from the upgrade payload. force should be used to force re-download of all URLs in the payload even if their zipfiles have been cached. An upgrade run is always required if the \"common\" pipelines in the base image change. These are updated for every run, and do not require force . See the job log to verify the exact actions taken by an upgrade job. Configuration Reference Web Server config This image contains both the backend and client. Variable Default Description GIRDER_MONGO_URI mongodb://mongo:27017/girder a mongodb connection string GIRDER_ADMIN_USER admin admin username GIRDER_ADMIN_PASS letmein admin password CELERY_BROKER_URL amqp://guest:guest@default/ rabbitmq connection string WORKER_API_URL http://girder:8080/api/v1 Address for workers to reach web server There is additional configuration for the RabbitMQ Management plugin. It only matters if you intend to allow individual users to configure private job runners in standalone mode, and can otherwise be ignored. Variable Default Description RABBITMQ_MANAGEMENT_USERNAME guest Management API username RABBITMQ_MANAGEMENT_PASSWORD guest Management API password RABBITMQ_MANAGEMENT_VHOST default Virtual host should match CELERY_BROKER_URL RABBITMQ_MANAGEMENT_URL http://rabbit:15672/ Management API Url You can also pass girder configuration and celery configuration . Worker config This image contains a celery worker to run VIAME pipelines and transcoding jobs. Note : Either a broker url or DIVE credentials must be supplied. Variable Default Description WORKER_WATCHING_QUEUES null one of celery , pipelines , training . Ignored in standalone mode. WORKER_CONCURRENCY # of CPU cores max concurrnet jobs. Lower this if you run training WORKER_GPU_UUID null leave empty to use all GPUs. Specify UUID to use specific device CELERY_BROKER_URL amqp://guest:guest@default/ rabbitmq connection string. Ignored in standalone mode. KWIVER_DEFAULT_LOG_LEVEL warn kwiver log level DIVE_USERNAME null Username to start private queue processor. Providing this enables standalone mode. DIVE_PASSWORD null Password for private queue processor. Providing this enables standalone mode. DIVE_API_URL https://viame.kitware.com/api/v1 Remote URL to authenticate against You can also pass regular celery configuration variables . Running the GPU Job Runner in standalone mode Linux Only. Individual users can run a standalone worker to process private jobs from VIAME Web. Install VIAME from the github page to /opt/noaa/viame . Activate the install with source setup_viame.sh . Install VIAME pipeline addons by running cd bin && download_viame_addons.sh from the VIAME install directory. Enable the private user queue for your jobs by visiting the jobs page Run a worker using the docker command below Note : The --volume mount maps to the host installation. You may need to change the source from /opt/noaa/viame depending on your install location, but you should not change the destination from /tmp/addons/extracted . 1 2 3 4 5 6 7 8 9 docker run --rm --name dive_worker \\ --gpus all \\ --ipc host \\ --volume \"/opt/noaa/viame/:/tmp/addons/extracted:ro\" \\ -e \"WORKER_CONCURRENCY=2\" \\ -e \"DIVE_USERNAME=CHANGEME\" \\ -e \"DIVE_PASSWORD=CHANGEME\" \\ -e \"DIVE_API_URL=https://viame.kitware.com/api/v1\" \\ kitware/viame-worker:latest","title":"Running with Docker Compose"},{"location":"Deployment-Docker-Compose/#running-with-docker-compose","text":"Start here once you have SSH access and sudo privileges for a server or VM. Note Docker server installation is only supported on Linux distributions","title":"Running with Docker Compose"},{"location":"Deployment-Docker-Compose/#container-images","text":"A DIVE Web deployment consists of 2 main services. kitware/viame-web - the web server kitware/viame-worker - the queue worker In addition, a database (MongoDB) and a queue service (RabbitMQ) are required.","title":"Container Images"},{"location":"Deployment-Docker-Compose/#install-dependencies","text":"SSH into the target server and install these system dependencies. Tip You can skip this section if you used Ansible to configure your server, as it already installed all necessary dependencies. Install NVIDIA driver version >= 450.80.02 and CUDA 11.0+ sudo ubuntu-drivers install Install docker version 19.03+ guide Install docker-compose version 1.28.0+ guide Install nvidia-container-toolkit","title":"Install dependencies"},{"location":"Deployment-Docker-Compose/#basic-deployment","text":"Clone this repository and configure options in .env . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Clone this repository git clone https://github.com/Kitware/dive /opt/dive # Change to correct directory cd /opt/dive # Initiate the .env file cp .env.default .env # Edit the .env file # See configuration options below and inline comments nano .env # Pull pre-built images docker-compose pull # Bring the services up docker-compose up -d VIAME server will be running at http://localhost:8010 . You should see a page that looks like this. The default username and password is admin:letmein .","title":"Basic deployment"},{"location":"Deployment-Docker-Compose/#production-deployment","text":"If you have a server with a public-facing IP address and a domain name that points to it, you should be able to use our production deployment configuration. This is the way we deploy viame.kitware.com. containrrr/watchtower updates the running containers on a schedule using automated image builds from docker hub (above). linuxserver/duplicati is included to schedule nightly backups, but must be manually configured. You should scale the girder web server up to an appropriate number. This stack will automatically load-balance across however many instances you bring up. 1 2 3 4 5 6 7 8 # Continuing from above, modify .env again to include the production variables nano .env # pull extra containers docker-compose -f docker-compose.yml -f docker-compose.prod.yml pull # scale the web service up docker-compose -f docker-compose.yml -f docker-compose.prod.yml up -d --scale girder = 4","title":"Production deployment"},{"location":"Deployment-Docker-Compose/#addon-management","text":"After initial deployment, a DIVE server will only have basic VIAME pipelines available. VIAME optional patches are installed and upgraded using a celery task that must be triggered by hand. Run this by issuing a POST /dive_configuration/upgrade_pipelines request from the swagger UI at http://{domain}/api/v1 . Whether you force or not, only those pipelines from addons from the exact urls passed will be enabled on the server. An old addon can be disabled by simply omitting its download from the upgrade payload. force should be used to force re-download of all URLs in the payload even if their zipfiles have been cached. An upgrade run is always required if the \"common\" pipelines in the base image change. These are updated for every run, and do not require force . See the job log to verify the exact actions taken by an upgrade job.","title":"Addon management"},{"location":"Deployment-Docker-Compose/#configuration-reference","text":"","title":"Configuration Reference"},{"location":"Deployment-Docker-Compose/#web-server-config","text":"This image contains both the backend and client. Variable Default Description GIRDER_MONGO_URI mongodb://mongo:27017/girder a mongodb connection string GIRDER_ADMIN_USER admin admin username GIRDER_ADMIN_PASS letmein admin password CELERY_BROKER_URL amqp://guest:guest@default/ rabbitmq connection string WORKER_API_URL http://girder:8080/api/v1 Address for workers to reach web server There is additional configuration for the RabbitMQ Management plugin. It only matters if you intend to allow individual users to configure private job runners in standalone mode, and can otherwise be ignored. Variable Default Description RABBITMQ_MANAGEMENT_USERNAME guest Management API username RABBITMQ_MANAGEMENT_PASSWORD guest Management API password RABBITMQ_MANAGEMENT_VHOST default Virtual host should match CELERY_BROKER_URL RABBITMQ_MANAGEMENT_URL http://rabbit:15672/ Management API Url You can also pass girder configuration and celery configuration .","title":"Web Server config"},{"location":"Deployment-Docker-Compose/#worker-config","text":"This image contains a celery worker to run VIAME pipelines and transcoding jobs. Note : Either a broker url or DIVE credentials must be supplied. Variable Default Description WORKER_WATCHING_QUEUES null one of celery , pipelines , training . Ignored in standalone mode. WORKER_CONCURRENCY # of CPU cores max concurrnet jobs. Lower this if you run training WORKER_GPU_UUID null leave empty to use all GPUs. Specify UUID to use specific device CELERY_BROKER_URL amqp://guest:guest@default/ rabbitmq connection string. Ignored in standalone mode. KWIVER_DEFAULT_LOG_LEVEL warn kwiver log level DIVE_USERNAME null Username to start private queue processor. Providing this enables standalone mode. DIVE_PASSWORD null Password for private queue processor. Providing this enables standalone mode. DIVE_API_URL https://viame.kitware.com/api/v1 Remote URL to authenticate against You can also pass regular celery configuration variables .","title":"Worker config"},{"location":"Deployment-Docker-Compose/#running-the-gpu-job-runner-in-standalone-mode","text":"Linux Only. Individual users can run a standalone worker to process private jobs from VIAME Web. Install VIAME from the github page to /opt/noaa/viame . Activate the install with source setup_viame.sh . Install VIAME pipeline addons by running cd bin && download_viame_addons.sh from the VIAME install directory. Enable the private user queue for your jobs by visiting the jobs page Run a worker using the docker command below Note : The --volume mount maps to the host installation. You may need to change the source from /opt/noaa/viame depending on your install location, but you should not change the destination from /tmp/addons/extracted . 1 2 3 4 5 6 7 8 9 docker run --rm --name dive_worker \\ --gpus all \\ --ipc host \\ --volume \"/opt/noaa/viame/:/tmp/addons/extracted:ro\" \\ -e \"WORKER_CONCURRENCY=2\" \\ -e \"DIVE_USERNAME=CHANGEME\" \\ -e \"DIVE_PASSWORD=CHANGEME\" \\ -e \"DIVE_API_URL=https://viame.kitware.com/api/v1\" \\ kitware/viame-worker:latest","title":"Running the GPU Job Runner in standalone mode"},{"location":"Deployment-Overview/","text":"Deployment Overview The goal of this page is to provide an overview of the ways to run VIAME or VIAME Web in various types of compute environments. Contents Using our deployment of VIAME Web Running your own instance of VIAME Web Using the VIAME command line and project folders in a cloud environment Hybrid options for using local or cloud compute resources with an existing deployment Hybrid options for integrating data from cloud storage such as GCP Buckets or S3 into an existing deployment Our server vs running your own Using our server Running your own Free to use; no maintenance costs You pay hosting and maintenance costs Always up to date Possible to configure automated updates One shared environment for everyone Your organization has full control over access Our team monitors this service for errors and can respond to issues proactively Support by email requires logs, screenshots, and other error information if applicable Our team can provide guidance on annotation and training because we have direct access to your data Support by email usually requires example data and annotations Having user data in our environment helps us understand user needs and improve the product Feedback by email is always appreciated. Limited shared compute resources (2 GPUs) available to process jobs. Can be mitigated by hybrid compute options As much compute as you pay for Using our public server The easiest option to get started using VIAME is to try our public server . Running your own instance You may wish to run your own deployment of VIAME Web in your lab or a cloud environment. Deploying VIAME Web is relatively straighforward with docker-compose . Environment Instructions Local server If you already have SSH access to an existing server and sudo permissions, proceed to the docker compose guide . Google Cloud Continue to the Provisioning Google Cloud page for Scenario 1 AWS / Azure Create a server on your own through the cloud management console, then proceed to the docker compose guide . VIAME CLI with project folders You may not want to use the web annotator and job orchestration at all, and instead run VIAME using the command line in a cloud environment with GPU. Environment Instructions Local server This is a standard VIAME install. See the VIAME documentation install instructions . Google Cloud Continue to the Provisioning Google Cloud page for Scenario 2 AWS / Azure Create a server through the cloud management console, then proceed to the VIAME documentation install instructions . Hybrid options for compute Instead of running the whole web stack, it's possible to deploy a worker by itself to process compute-intensive jobs. This is referred to in the docs as standalone mode . For example, you could: Upload and annotate at viame.kitware.com, but run your own private worker on a lab workstation Deploy your own web server to a local lab workstation, but process your jobs in an ephemeral Google Cloud VM. How it works You must toggle your private queue When you launch jobs (like transcoding, pipelines, or training), they go into a special queue just for your user account. You are responsible for running a worker. Your worker is a Celery process that will connect to our public RabbitMQ server. Jobs submitted through the interface at viame.kitware.com will run on your compute resources. This involves automatically downloading the video or images and annotation files, running a kwiver pipeline, and uploading the results. To set up a private worker, continue to the Provisioning Google Cloud page for Scenario 3 . Hybrid options for storage Any instance of VIAME Web, including our public server, can connect to S3-compatible storage. This means your lab or group could make your existing data available at viame.kitware.com , either privately or publicly. Storage Product Support level Google Cloud Buckets Use as backing storage, import existing data, monitor for changes and automatically discover new uploads AWS S3 Use as backing storage, import existing data MinIO Use as backing storage, import existing data Azure Blob Storage Limited import support using MinIO Azure Gateway Get Help Contact us for support with any of these topics.","title":"Deployment Options Overview"},{"location":"Deployment-Overview/#deployment-overview","text":"The goal of this page is to provide an overview of the ways to run VIAME or VIAME Web in various types of compute environments.","title":"Deployment Overview"},{"location":"Deployment-Overview/#contents","text":"Using our deployment of VIAME Web Running your own instance of VIAME Web Using the VIAME command line and project folders in a cloud environment Hybrid options for using local or cloud compute resources with an existing deployment Hybrid options for integrating data from cloud storage such as GCP Buckets or S3 into an existing deployment","title":"Contents"},{"location":"Deployment-Overview/#our-server-vs-running-your-own","text":"Using our server Running your own Free to use; no maintenance costs You pay hosting and maintenance costs Always up to date Possible to configure automated updates One shared environment for everyone Your organization has full control over access Our team monitors this service for errors and can respond to issues proactively Support by email requires logs, screenshots, and other error information if applicable Our team can provide guidance on annotation and training because we have direct access to your data Support by email usually requires example data and annotations Having user data in our environment helps us understand user needs and improve the product Feedback by email is always appreciated. Limited shared compute resources (2 GPUs) available to process jobs. Can be mitigated by hybrid compute options As much compute as you pay for","title":"Our server vs running your own"},{"location":"Deployment-Overview/#using-our-public-server","text":"The easiest option to get started using VIAME is to try our public server .","title":"Using our public server"},{"location":"Deployment-Overview/#running-your-own-instance","text":"You may wish to run your own deployment of VIAME Web in your lab or a cloud environment. Deploying VIAME Web is relatively straighforward with docker-compose . Environment Instructions Local server If you already have SSH access to an existing server and sudo permissions, proceed to the docker compose guide . Google Cloud Continue to the Provisioning Google Cloud page for Scenario 1 AWS / Azure Create a server on your own through the cloud management console, then proceed to the docker compose guide .","title":"Running your own instance"},{"location":"Deployment-Overview/#viame-cli-with-project-folders","text":"You may not want to use the web annotator and job orchestration at all, and instead run VIAME using the command line in a cloud environment with GPU. Environment Instructions Local server This is a standard VIAME install. See the VIAME documentation install instructions . Google Cloud Continue to the Provisioning Google Cloud page for Scenario 2 AWS / Azure Create a server through the cloud management console, then proceed to the VIAME documentation install instructions .","title":"VIAME CLI with project folders"},{"location":"Deployment-Overview/#hybrid-options-for-compute","text":"Instead of running the whole web stack, it's possible to deploy a worker by itself to process compute-intensive jobs. This is referred to in the docs as standalone mode . For example, you could: Upload and annotate at viame.kitware.com, but run your own private worker on a lab workstation Deploy your own web server to a local lab workstation, but process your jobs in an ephemeral Google Cloud VM. How it works You must toggle your private queue When you launch jobs (like transcoding, pipelines, or training), they go into a special queue just for your user account. You are responsible for running a worker. Your worker is a Celery process that will connect to our public RabbitMQ server. Jobs submitted through the interface at viame.kitware.com will run on your compute resources. This involves automatically downloading the video or images and annotation files, running a kwiver pipeline, and uploading the results. To set up a private worker, continue to the Provisioning Google Cloud page for Scenario 3 .","title":"Hybrid options for compute"},{"location":"Deployment-Overview/#hybrid-options-for-storage","text":"Any instance of VIAME Web, including our public server, can connect to S3-compatible storage. This means your lab or group could make your existing data available at viame.kitware.com , either privately or publicly. Storage Product Support level Google Cloud Buckets Use as backing storage, import existing data, monitor for changes and automatically discover new uploads AWS S3 Use as backing storage, import existing data MinIO Use as backing storage, import existing data Azure Blob Storage Limited import support using MinIO Azure Gateway","title":"Hybrid options for storage"},{"location":"Deployment-Overview/#get-help","text":"Contact us for support with any of these topics.","title":"Get Help"},{"location":"Deployment-Provision/","text":"Cloud Deployment Guide Note Be sure to read the Deployment Overview first. Scenario 1 : Deploy your own instance of VIAME Web to GCP Compute Engine. Scenario 2 : Run VIAME pipelines on a GCP Compute Engine VM from the command line. Scenario 3 : Run a Private GPU worker in GCP to process jobs from any VIAME Web instance including viame.kitware.com (standalone mode) The terraform section is the same for all scenarios. The Ansible section will have differences. Before you begin You'll need a GCP Virtual Machine (VM) with the features listed below. This section will guide you through creating one and deploying VIAME using Terraform and Ansible. Terraform automates the process of creating and destroying cloud resources such as VMs. Ansible automates configuration, such as software installation, on newly created machines. Together, these tools allow you to quickly create a reproducible environment. If you do not want to use these tools, you can create your own VM manually through the management console and skip to the docker documentation instead . Feature Recommended value Operating system Ubuntu 20.04 Instance Type n1-standard-4 or larger GPU Type nvidia-tesla-t4 , nvidia-tesla-p4 , or similar Disk Type SSD, 128GB or more depending on your needs Install dependencies To run the provisioning tools below, you need the following installed on your own workstation. Note Google Cloud worker provisioning can only be done from an Ubuntu Linux 18.04+ host. Ansible and terraform should work on Windows Subsystem for Linux (WSL) if you only have a windows host. You could also use a cheap CPU-only cloud instance to run these tools. Install Google Cloud SDK Install Terraform Install Ansible Find your google cloud project id. It looks like project-name-123456 . Tip Google Cloud imposes GPU Quotas. You may need to request a quota increase . Anecdotally, request increases of 1 unit are approved automatically, but more are rejected. Creating a VM with Terraform 1 2 3 4 5 6 # Clone the dive repo git clone https://github.com/Kitware/dive.git cd dive/devops # Generate a new ssh key ssh-keygen -t ed25519 -f ~/.ssh/gcloud_key Run Terraform Warning GPU-accelerated VMs are significantly more expensive than typical VMs. Make sure you are familiar with the cost of the machine and GPU you choose. See main.tf for default values. See devops/main.tf for a complete list of variables. The default machine_type , gpu_type , and gpu_count can be overridden. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # Authenticate with google cloud gcloud auth application-default login # Verify your GPU Quota # https://cloud.google.com/compute/docs/gpus/create-vm-with-gpus # REGION might change. gcloud compute regions describe us-central1 # Run plan, providing any variables you choose terraform plan \\ -var \"project_name=<GCloud-Project-Id>\" \\ -var \"gpu_count=1\" \\ -out create.plan # Run apply. It may take several minutes terraform apply create.plan Destroy the stack Later, when you are done with the server and have backed up your data, use terraform to destroy your resources. 1 terraform destroy -var \"project_name=<GCloud-Project-Id>\" Configure with Ansible This step will prepare the new host to run a VIAME worker by installing nvidia drivers, docker, and downloading VIAME and all optional addons. Warning The playbook may take 30 minutes or more to run because it must install nvidia drivers and download several GB of software packages. Ansible Extra Vars These are all the variables that can be provided with --extra-vars along with which scenario each on applies to. Variable Default Description run_server no Set run_server=yes for scenario 1 (Web Instance) run_viame_cli no Set run_viame_cli=yes for scenario 2 (VIAME CLI) run_worker_container no Set run_worker_container=yes for scenario 3 (Standalone Worker) viame_bundle_url latest bundle url Optional for scenario 2 & 3. Change to install a different version of VIAME. This should be a link to the latest Ubuntu Desktop (18/20) binaries from viame.kitware.com (Mirror 1) DIVE_USERNAME null Required for scenario 3. Username to start private queue processor DIVE_PASSWORD null Required for scenario 3. Password for private queue processor WORKER_CONCURRENCY 2 Optional for scenario 3. Max concurrnet jobs. Change this to 1 if you run training DIVE_API_URL https://viame.kitware.com/api/v1 Optional for scenario 3. Remote URL to authenticate against. KWIVER_DEFAULT_LOG_LEVEL warn Optional for scenario 3. kwiver log level Run Ansible The examples below assumes the inventory file was created by Terraform above. 1 2 3 4 5 6 7 8 9 10 11 12 13 # install galaxy plugins ansible-galaxy install -r ansible/requirements.yml # Choose only 1 of the scenarios below # Scenario 1 (Web Instance) Example ansible-playbook -i inventory ansible/playbook.yml --extra-vars \"run_server=yes\" # Scenario 2 (VIAME CLI) Example ansible-playbook -i inventory ansible/playbook.yml --extra-vars \"run_viame_cli=yes\" # Scenario 3 (Standalone Worker) Example ansible-playbook -i inventory ansible/playbook.yml --extra-vars \"run_worker_container=yes DIVE_USERNAME=username DIVE_PASSWORD=changeme\" Once provisioning is complete, jobs should begin processing from the job queue. You can check viame.kitware.com/#/jobs to see queue progress and logs. Tip This ansible playbook is runnable from any Ubuntu 18.04+ host to any Ubuntu 18.04+ target. To run it locally, use the inventory.local file instead. If you already have nvidia or docker installed, you can comment out these lines in the playbook. 1 ansible-playbook --ask-become-pass -i inventory ansible/playbook.yml --extra-vars \"<see above>\" Check that it worked 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # Log in with the ip address from the inventory file (or google cloud dashboard) ssh -i ~/.ssh/gcloud_key viame@ip-address # Test nvidia docker installation docker run --gpus = all --rm nvidia/cuda nvidia-smi # Test regular nvidia runtime nvidia-smi # For Scenario 2 and 3, check KWIVER installation cd /opt/noaa/viame source setup_viame.sh kwiver # For Scenario 3, you can check to see if the worker is started # You should see \"celery@identifier ready.\" in the logs sudo docker logs -f worker You can enable your private queue on the jobs page and begin running jobs. Next Steps Scenario 1 : Proceed to Docker Compose Deployment . Scenario 2 : Setup is complete. Proceed to the VIAME Documentation . Scenario 3 : Setup is complete. Make sure your private queue is enabled. Troubleshooting Ansible provisioning is idempotent. If it fails, run it again once or twice. You may need to change the global GPUS_ALL_REGIONS quota in IAM -> Quotas","title":"Provisioning Google Cloud"},{"location":"Deployment-Provision/#cloud-deployment-guide","text":"Note Be sure to read the Deployment Overview first. Scenario 1 : Deploy your own instance of VIAME Web to GCP Compute Engine. Scenario 2 : Run VIAME pipelines on a GCP Compute Engine VM from the command line. Scenario 3 : Run a Private GPU worker in GCP to process jobs from any VIAME Web instance including viame.kitware.com (standalone mode) The terraform section is the same for all scenarios. The Ansible section will have differences.","title":"Cloud Deployment Guide"},{"location":"Deployment-Provision/#before-you-begin","text":"You'll need a GCP Virtual Machine (VM) with the features listed below. This section will guide you through creating one and deploying VIAME using Terraform and Ansible. Terraform automates the process of creating and destroying cloud resources such as VMs. Ansible automates configuration, such as software installation, on newly created machines. Together, these tools allow you to quickly create a reproducible environment. If you do not want to use these tools, you can create your own VM manually through the management console and skip to the docker documentation instead . Feature Recommended value Operating system Ubuntu 20.04 Instance Type n1-standard-4 or larger GPU Type nvidia-tesla-t4 , nvidia-tesla-p4 , or similar Disk Type SSD, 128GB or more depending on your needs","title":"Before you begin"},{"location":"Deployment-Provision/#install-dependencies","text":"To run the provisioning tools below, you need the following installed on your own workstation. Note Google Cloud worker provisioning can only be done from an Ubuntu Linux 18.04+ host. Ansible and terraform should work on Windows Subsystem for Linux (WSL) if you only have a windows host. You could also use a cheap CPU-only cloud instance to run these tools. Install Google Cloud SDK Install Terraform Install Ansible Find your google cloud project id. It looks like project-name-123456 . Tip Google Cloud imposes GPU Quotas. You may need to request a quota increase . Anecdotally, request increases of 1 unit are approved automatically, but more are rejected.","title":"Install dependencies"},{"location":"Deployment-Provision/#creating-a-vm-with-terraform","text":"1 2 3 4 5 6 # Clone the dive repo git clone https://github.com/Kitware/dive.git cd dive/devops # Generate a new ssh key ssh-keygen -t ed25519 -f ~/.ssh/gcloud_key","title":"Creating a VM with Terraform"},{"location":"Deployment-Provision/#run-terraform","text":"Warning GPU-accelerated VMs are significantly more expensive than typical VMs. Make sure you are familiar with the cost of the machine and GPU you choose. See main.tf for default values. See devops/main.tf for a complete list of variables. The default machine_type , gpu_type , and gpu_count can be overridden. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # Authenticate with google cloud gcloud auth application-default login # Verify your GPU Quota # https://cloud.google.com/compute/docs/gpus/create-vm-with-gpus # REGION might change. gcloud compute regions describe us-central1 # Run plan, providing any variables you choose terraform plan \\ -var \"project_name=<GCloud-Project-Id>\" \\ -var \"gpu_count=1\" \\ -out create.plan # Run apply. It may take several minutes terraform apply create.plan","title":"Run Terraform"},{"location":"Deployment-Provision/#destroy-the-stack","text":"Later, when you are done with the server and have backed up your data, use terraform to destroy your resources. 1 terraform destroy -var \"project_name=<GCloud-Project-Id>\"","title":"Destroy the stack"},{"location":"Deployment-Provision/#configure-with-ansible","text":"This step will prepare the new host to run a VIAME worker by installing nvidia drivers, docker, and downloading VIAME and all optional addons. Warning The playbook may take 30 minutes or more to run because it must install nvidia drivers and download several GB of software packages.","title":"Configure with Ansible"},{"location":"Deployment-Provision/#ansible-extra-vars","text":"These are all the variables that can be provided with --extra-vars along with which scenario each on applies to. Variable Default Description run_server no Set run_server=yes for scenario 1 (Web Instance) run_viame_cli no Set run_viame_cli=yes for scenario 2 (VIAME CLI) run_worker_container no Set run_worker_container=yes for scenario 3 (Standalone Worker) viame_bundle_url latest bundle url Optional for scenario 2 & 3. Change to install a different version of VIAME. This should be a link to the latest Ubuntu Desktop (18/20) binaries from viame.kitware.com (Mirror 1) DIVE_USERNAME null Required for scenario 3. Username to start private queue processor DIVE_PASSWORD null Required for scenario 3. Password for private queue processor WORKER_CONCURRENCY 2 Optional for scenario 3. Max concurrnet jobs. Change this to 1 if you run training DIVE_API_URL https://viame.kitware.com/api/v1 Optional for scenario 3. Remote URL to authenticate against. KWIVER_DEFAULT_LOG_LEVEL warn Optional for scenario 3. kwiver log level","title":"Ansible Extra Vars"},{"location":"Deployment-Provision/#run-ansible","text":"The examples below assumes the inventory file was created by Terraform above. 1 2 3 4 5 6 7 8 9 10 11 12 13 # install galaxy plugins ansible-galaxy install -r ansible/requirements.yml # Choose only 1 of the scenarios below # Scenario 1 (Web Instance) Example ansible-playbook -i inventory ansible/playbook.yml --extra-vars \"run_server=yes\" # Scenario 2 (VIAME CLI) Example ansible-playbook -i inventory ansible/playbook.yml --extra-vars \"run_viame_cli=yes\" # Scenario 3 (Standalone Worker) Example ansible-playbook -i inventory ansible/playbook.yml --extra-vars \"run_worker_container=yes DIVE_USERNAME=username DIVE_PASSWORD=changeme\" Once provisioning is complete, jobs should begin processing from the job queue. You can check viame.kitware.com/#/jobs to see queue progress and logs. Tip This ansible playbook is runnable from any Ubuntu 18.04+ host to any Ubuntu 18.04+ target. To run it locally, use the inventory.local file instead. If you already have nvidia or docker installed, you can comment out these lines in the playbook. 1 ansible-playbook --ask-become-pass -i inventory ansible/playbook.yml --extra-vars \"<see above>\"","title":"Run Ansible"},{"location":"Deployment-Provision/#check-that-it-worked","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # Log in with the ip address from the inventory file (or google cloud dashboard) ssh -i ~/.ssh/gcloud_key viame@ip-address # Test nvidia docker installation docker run --gpus = all --rm nvidia/cuda nvidia-smi # Test regular nvidia runtime nvidia-smi # For Scenario 2 and 3, check KWIVER installation cd /opt/noaa/viame source setup_viame.sh kwiver # For Scenario 3, you can check to see if the worker is started # You should see \"celery@identifier ready.\" in the logs sudo docker logs -f worker You can enable your private queue on the jobs page and begin running jobs.","title":"Check that it worked"},{"location":"Deployment-Provision/#next-steps","text":"Scenario 1 : Proceed to Docker Compose Deployment . Scenario 2 : Setup is complete. Proceed to the VIAME Documentation . Scenario 3 : Setup is complete. Make sure your private queue is enabled.","title":"Next Steps"},{"location":"Deployment-Provision/#troubleshooting","text":"Ansible provisioning is idempotent. If it fails, run it again once or twice. You may need to change the global GPUS_ALL_REGIONS quota in IAM -> Quotas","title":"Troubleshooting"},{"location":"Deployment-Storage/","text":"Cloud Storage Integration This page is intended for storage administrators who would like to make their existing data available through VIAME Web. Tip This guide assumes you are working with viame.kitware.com . If you are using a different deployment, be sure to change the appropriate fields. Tip Regarding data transfer costs, if you choose to keep both your data storage and job runners in Google Cloud (or AWS), you will avoid paying a data egress fee for transferring data between storage and the processing node. Google Cloud Storage Mirroring DIVE Web can mirror your data from Google Cloud storage buckets such that your team fully controls upload and data organization, but is able to view, annotate, and run analysis within VIAME Web. Creating access credentials Create a new service account with read-only access to the bucket(s) and prefixes that you want to map. In storage settings , in the interoperability tab, create an access key (Service account HMAC) for your read-only service account. Setting up CORS You'll also need to configure CORS headers for any buckets where media will be served. Save the following snippet as bucket-cors-config.json . 1 2 3 4 5 6 7 8 [ { \"origin\" : [ \"https://viame.kitware.com\" ], \"method\" : [ \"GET\" ], \"responseHeader\" : [ \"Content-Type\" ], \"maxAgeSeconds\" : 3600 } ] Then use gsutils to configure each bucket. 1 gsutil cors set bucket-cors-config.json gs://BUCKET_NAME Pub/Sub notifications To keep the mount up-to-date with new data added to your bucket, please create a Pub/Sub subscription on the bucket. Create a bucket notification configuration Create a topic subscription Set a push delivery method for the subsciption The URL for delivery should be https://viame.kitware.com/api/v1/bucket_notifications/gcs Our server will process events from this subscription to keep your data current. Choose a mount point Choose a folder as a mount-point inside DIVE Web. This folder should ideally be dedicated to mapping from your GCS buckets. We recommend creating a Google Cloud Storage folder with subfolders named for each bucket in your user's workspace. You can do this using the New Folder button in DIVE Web's File Browser. You can get the folder ID from the browser's URL bar. Send us the details Send an email with the following details from above to viame-web@kitware.com . 1 2 3 4 5 6 7 8 subject: Add a google cloud storage bucket mount Bucket name: Service provider: Google cloud Access Key: Secret Key: Mount point folder: Prefix (if applicable):","title":"Cloud Storage Integration"},{"location":"Deployment-Storage/#cloud-storage-integration","text":"This page is intended for storage administrators who would like to make their existing data available through VIAME Web. Tip This guide assumes you are working with viame.kitware.com . If you are using a different deployment, be sure to change the appropriate fields. Tip Regarding data transfer costs, if you choose to keep both your data storage and job runners in Google Cloud (or AWS), you will avoid paying a data egress fee for transferring data between storage and the processing node.","title":"Cloud Storage Integration"},{"location":"Deployment-Storage/#google-cloud-storage-mirroring","text":"DIVE Web can mirror your data from Google Cloud storage buckets such that your team fully controls upload and data organization, but is able to view, annotate, and run analysis within VIAME Web.","title":"Google Cloud Storage Mirroring"},{"location":"Deployment-Storage/#creating-access-credentials","text":"Create a new service account with read-only access to the bucket(s) and prefixes that you want to map. In storage settings , in the interoperability tab, create an access key (Service account HMAC) for your read-only service account.","title":"Creating access credentials"},{"location":"Deployment-Storage/#setting-up-cors","text":"You'll also need to configure CORS headers for any buckets where media will be served. Save the following snippet as bucket-cors-config.json . 1 2 3 4 5 6 7 8 [ { \"origin\" : [ \"https://viame.kitware.com\" ], \"method\" : [ \"GET\" ], \"responseHeader\" : [ \"Content-Type\" ], \"maxAgeSeconds\" : 3600 } ] Then use gsutils to configure each bucket. 1 gsutil cors set bucket-cors-config.json gs://BUCKET_NAME","title":"Setting up CORS"},{"location":"Deployment-Storage/#pubsub-notifications","text":"To keep the mount up-to-date with new data added to your bucket, please create a Pub/Sub subscription on the bucket. Create a bucket notification configuration Create a topic subscription Set a push delivery method for the subsciption The URL for delivery should be https://viame.kitware.com/api/v1/bucket_notifications/gcs Our server will process events from this subscription to keep your data current.","title":"Pub/Sub notifications"},{"location":"Deployment-Storage/#choose-a-mount-point","text":"Choose a folder as a mount-point inside DIVE Web. This folder should ideally be dedicated to mapping from your GCS buckets. We recommend creating a Google Cloud Storage folder with subfolders named for each bucket in your user's workspace. You can do this using the New Folder button in DIVE Web's File Browser. You can get the folder ID from the browser's URL bar.","title":"Choose a mount point"},{"location":"Deployment-Storage/#send-us-the-details","text":"Send an email with the following details from above to viame-web@kitware.com . 1 2 3 4 5 6 7 8 subject: Add a google cloud storage bucket mount Bucket name: Service provider: Google cloud Access Key: Secret Key: Mount point folder: Prefix (if applicable):","title":"Send us the details"},{"location":"Dive-Desktop/","text":"DIVE Desktop DIVE is available as an electron based desktop application with deep VIAME integration. DIVE Desktop has most of the same UI and features as DIVE without requiring a network connection or a server installation. Installation \u2b07\ufe0f Download the latest DIVE Desktop from GitHub Choose an asset from the list matching your operating system: OS Extension Description Windows .exe Portable executable (recommended) Windows .msi Installer file MacOS .dmg MacOS DiskImage (Intel only, M1 not supported) Linux .AppImage Portable executable for all Linux platforms (recommended) Linux .snap Ubuntu SnapCraft package Full VIAME Desktop Installation This is just the installation guide for DIVE. If you want the full VIAME tool suite, you can get it from github.com/viame/viame Features Full Windows and Linux support. Annotation support for MacOS. Annotate video and images on your computer (Instead of uploading to a server) Run pipelines and training on multiple datasets using locally installed VIAME Importing Images & Videos For video, DIVE will ask you to point directly to a file. For images, DIVE Desktop imports entire directories . That means all images from a single folder will be imported as a dataset. You can use globbing to filter the contents of an image directory during import. Annotation Files - In either case, either a *.csv or a result*.json annotaton file should be located in the same directory as the source media, and will be automatically discovered during import. Annotation files are not required. Video Transcoding DIVE Desktop is an Electron application built on web technologies. Certain video codecs require automatic transcoding to be usable. Video will be transcoded unless all the following conditions are met. codec = h264 sample_aspect_ratio (SAR) = 1:1 Configuration DIVE Desktop requires a local installation of the VIAME toolkit to run pipelines, train, and do transcoding. VIAME Install Path is set automatically if you use examples/annotation_and_visualization/launch_dive_interface from the VIAME install. Otherwise, you may need to set this yourself. Use Choose to choose the base installation path, then click save. Project Data Storage Path defaults to a subfolder in your user workspace and should generally not be changed. Read only mode disables the ability to save when using the annotator. Data Storage Path The data storage path is not at all related to \"project folders\" in VIAME. It's just a place for DIVE Desktop to keep and structure all the data it needs to run. A typical data storage directory has 3 subfolders: DIVE_Jobs - Each job run has a working directory, kept here. DIVE_Projects - Each dataset you import into desktop has metadata and annotation data (with revision history) kept here. DIVE_Pipelines - Training runs produce models that get copied into here. Here's an example of structure you might find in the storage path. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 VIAME_DATA \u251c\u2500\u2500 DIVE_Jobs \u2502 \u251c\u2500\u2500 Scallop_1_scallop and flatfish_06-01-2021_11-02-11.585 \u2502 \u2502 \u251c\u2500\u2500 detector_output.csv \u2502 \u2502 \u251c\u2500\u2500 dive_job_manifest.json \u2502 \u2502 \u251c\u2500\u2500 image-manifest.txt \u2502 \u2502 \u2514\u2500\u2500 runlog.txt \u2502 \u2514\u2500\u2500 Scallop_2_scallop netharn_06-01-2021_11-02-19.432 \u2502 \u251c\u2500\u2500 detector_output.csv \u2502 \u251c\u2500\u2500 dive_job_manifest.json \u2502 \u251c\u2500\u2500 image-manifest.txt \u2502 \u2514\u2500\u2500 runlog.txt \u251c\u2500\u2500 DIVE_Pipelines \u2502 \u251c\u2500\u2500 My Fish SVM Demo \u2502 \u2502 \u251c\u2500\u2500 detector.pipe \u2502 \u2502 \u2514\u2500\u2500 fish.svm \u2502 \u2514\u2500\u2500 Quadcam_Fish_Detector_SVM \u2502 \u251c\u2500\u2500 detector.pipe \u2502 \u2514\u2500\u2500 fish.svm \u2514\u2500\u2500 DIVE_Projects \u251c\u2500\u2500 fish_training_data_c_jp7hq88vfv \u2502 \u251c\u2500\u2500 auxiliary \u2502 \u2502 \u2514\u2500\u2500 result_06-01-2021_10-55-38.627.json \u2502 \u251c\u2500\u2500 meta.json \u2502 \u2514\u2500\u2500 result_06-01-2021_04-53-38.050.json \u2514\u2500\u2500 scallop_2_jrgdq760gu \u251c\u2500\u2500 auxiliary \u2502 \u2514\u2500\u2500 result_06-01-2021_10-54-56.034.json \u251c\u2500\u2500 meta.json \u2514\u2500\u2500 result_06-01-2021_11-02-35.857.json Configuration with env DIVE Desktop looks for the these environment variables on launch. Name Default Description DIVE_VIAME_INSTALL_PATH /opt/noaa/viame (Linux/macOS) C:\\Program Files\\VIAME (Windows) Overrides the location of the VIAME installation. Users may not change this value in the settings pane if provided. DIVE_READONLY_MODE None Overrides read only mode to true or false. Users may still change this value in the settings pane if provided. Import/Export of Models Trained models are kept in ${Project Data Storage Path}/DIVE_Pipelines as described above. Each model file consists of exactly 1 pipe file and some number of other model files. The pipe file can be one of detector.pipe , tracker.pipe , or generate.pipe . Other files can be .zip , .svm , .lbl , or .cfg . You can use exteranally trained models in DIVE by creating a folder containing these files. The name of the configuration or pipeline in dive will be the folder name you create. Troubleshooting I imported some data, but I don't see my annotations See Importing images and video above . ffmpeg not installed, please download and install VIAME Toolkit from the main page DIVE Desktop relies on an installation of ffmpeg for transcoding videos and some images. This tool comes with the VIAME installation. Verify your VIAME Install Base Path is correct. Some VIAME canned pipelines are missing? If you don't see some pipelines you expect, you may not have installed the addons (also called Optional Patches) yet. Download and install these based on the VIAME installation docs . Advanced troubleshooting If you're experience problems or have questions about DIVE Desktop, contact us and include the content from the settings page such as Build Version as well as your currently installed VIAME version. To help us address errors and exceptions, it's helpful to look in the debug console. Press CTRL + SHIFT + i to launch the Dev Tools and look under the console tab. Errors and warnings will appear in red and yellow. You can right-click in the console area and click \"Save As\" to save the log file to email to us.","title":"Desktop Version"},{"location":"Dive-Desktop/#dive-desktop","text":"DIVE is available as an electron based desktop application with deep VIAME integration. DIVE Desktop has most of the same UI and features as DIVE without requiring a network connection or a server installation.","title":"DIVE Desktop"},{"location":"Dive-Desktop/#installation","text":"\u2b07\ufe0f Download the latest DIVE Desktop from GitHub Choose an asset from the list matching your operating system: OS Extension Description Windows .exe Portable executable (recommended) Windows .msi Installer file MacOS .dmg MacOS DiskImage (Intel only, M1 not supported) Linux .AppImage Portable executable for all Linux platforms (recommended) Linux .snap Ubuntu SnapCraft package","title":"Installation"},{"location":"Dive-Desktop/#full-viame-desktop-installation","text":"This is just the installation guide for DIVE. If you want the full VIAME tool suite, you can get it from github.com/viame/viame","title":"Full VIAME Desktop Installation"},{"location":"Dive-Desktop/#features","text":"Full Windows and Linux support. Annotation support for MacOS. Annotate video and images on your computer (Instead of uploading to a server) Run pipelines and training on multiple datasets using locally installed VIAME","title":"Features"},{"location":"Dive-Desktop/#importing-images-videos","text":"For video, DIVE will ask you to point directly to a file. For images, DIVE Desktop imports entire directories . That means all images from a single folder will be imported as a dataset. You can use globbing to filter the contents of an image directory during import. Annotation Files - In either case, either a *.csv or a result*.json annotaton file should be located in the same directory as the source media, and will be automatically discovered during import. Annotation files are not required.","title":"Importing Images &amp; Videos"},{"location":"Dive-Desktop/#video-transcoding","text":"DIVE Desktop is an Electron application built on web technologies. Certain video codecs require automatic transcoding to be usable. Video will be transcoded unless all the following conditions are met. codec = h264 sample_aspect_ratio (SAR) = 1:1","title":"Video Transcoding"},{"location":"Dive-Desktop/#configuration","text":"DIVE Desktop requires a local installation of the VIAME toolkit to run pipelines, train, and do transcoding. VIAME Install Path is set automatically if you use examples/annotation_and_visualization/launch_dive_interface from the VIAME install. Otherwise, you may need to set this yourself. Use Choose to choose the base installation path, then click save. Project Data Storage Path defaults to a subfolder in your user workspace and should generally not be changed. Read only mode disables the ability to save when using the annotator.","title":"Configuration"},{"location":"Dive-Desktop/#data-storage-path","text":"The data storage path is not at all related to \"project folders\" in VIAME. It's just a place for DIVE Desktop to keep and structure all the data it needs to run. A typical data storage directory has 3 subfolders: DIVE_Jobs - Each job run has a working directory, kept here. DIVE_Projects - Each dataset you import into desktop has metadata and annotation data (with revision history) kept here. DIVE_Pipelines - Training runs produce models that get copied into here. Here's an example of structure you might find in the storage path. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 VIAME_DATA \u251c\u2500\u2500 DIVE_Jobs \u2502 \u251c\u2500\u2500 Scallop_1_scallop and flatfish_06-01-2021_11-02-11.585 \u2502 \u2502 \u251c\u2500\u2500 detector_output.csv \u2502 \u2502 \u251c\u2500\u2500 dive_job_manifest.json \u2502 \u2502 \u251c\u2500\u2500 image-manifest.txt \u2502 \u2502 \u2514\u2500\u2500 runlog.txt \u2502 \u2514\u2500\u2500 Scallop_2_scallop netharn_06-01-2021_11-02-19.432 \u2502 \u251c\u2500\u2500 detector_output.csv \u2502 \u251c\u2500\u2500 dive_job_manifest.json \u2502 \u251c\u2500\u2500 image-manifest.txt \u2502 \u2514\u2500\u2500 runlog.txt \u251c\u2500\u2500 DIVE_Pipelines \u2502 \u251c\u2500\u2500 My Fish SVM Demo \u2502 \u2502 \u251c\u2500\u2500 detector.pipe \u2502 \u2502 \u2514\u2500\u2500 fish.svm \u2502 \u2514\u2500\u2500 Quadcam_Fish_Detector_SVM \u2502 \u251c\u2500\u2500 detector.pipe \u2502 \u2514\u2500\u2500 fish.svm \u2514\u2500\u2500 DIVE_Projects \u251c\u2500\u2500 fish_training_data_c_jp7hq88vfv \u2502 \u251c\u2500\u2500 auxiliary \u2502 \u2502 \u2514\u2500\u2500 result_06-01-2021_10-55-38.627.json \u2502 \u251c\u2500\u2500 meta.json \u2502 \u2514\u2500\u2500 result_06-01-2021_04-53-38.050.json \u2514\u2500\u2500 scallop_2_jrgdq760gu \u251c\u2500\u2500 auxiliary \u2502 \u2514\u2500\u2500 result_06-01-2021_10-54-56.034.json \u251c\u2500\u2500 meta.json \u2514\u2500\u2500 result_06-01-2021_11-02-35.857.json","title":"Data Storage Path"},{"location":"Dive-Desktop/#configuration-with-env","text":"DIVE Desktop looks for the these environment variables on launch. Name Default Description DIVE_VIAME_INSTALL_PATH /opt/noaa/viame (Linux/macOS) C:\\Program Files\\VIAME (Windows) Overrides the location of the VIAME installation. Users may not change this value in the settings pane if provided. DIVE_READONLY_MODE None Overrides read only mode to true or false. Users may still change this value in the settings pane if provided.","title":"Configuration with env"},{"location":"Dive-Desktop/#importexport-of-models","text":"Trained models are kept in ${Project Data Storage Path}/DIVE_Pipelines as described above. Each model file consists of exactly 1 pipe file and some number of other model files. The pipe file can be one of detector.pipe , tracker.pipe , or generate.pipe . Other files can be .zip , .svm , .lbl , or .cfg . You can use exteranally trained models in DIVE by creating a folder containing these files. The name of the configuration or pipeline in dive will be the folder name you create.","title":"Import/Export of Models"},{"location":"Dive-Desktop/#troubleshooting","text":"I imported some data, but I don't see my annotations See Importing images and video above . ffmpeg not installed, please download and install VIAME Toolkit from the main page DIVE Desktop relies on an installation of ffmpeg for transcoding videos and some images. This tool comes with the VIAME installation. Verify your VIAME Install Base Path is correct. Some VIAME canned pipelines are missing? If you don't see some pipelines you expect, you may not have installed the addons (also called Optional Patches) yet. Download and install these based on the VIAME installation docs . Advanced troubleshooting If you're experience problems or have questions about DIVE Desktop, contact us and include the content from the settings page such as Build Version as well as your currently installed VIAME version. To help us address errors and exceptions, it's helpful to look in the debug console. Press CTRL + SHIFT + i to launch the Dev Tools and look under the console tab. Errors and warnings will appear in red and yellow. You can right-click in the console area and click \"Save As\" to save the log file to email to us.","title":"Troubleshooting"},{"location":"FAQ/","text":"Frequently Asked Questions How do I find existing data to use? There are two places to look. The Training Data Collection is organized rougly by domain and collection method. The Stats Summary Page lists every published dataset organized by object label. How do I share data with others? This use case is covered on the sharing page . If you want to publish your data so that other groups can use it, please email viame-web@kitware.com . How do I run analysis workflows on my data? In DIVE, these are called pipelines. You'll need to see what sorts of analysis workflows are currently available on the pipeline page . These sorts of AI workflows are the final goal for most users. They allow the user to quickly perform quantitative analysis to answer questions like how many individuals of each type appear on each image or video frame? If no suitable existing analysis exists for your use case or you aren't sure how to proceed, you're welcome to email our team and ask for help . How do I create new models? You want to perform analysis (detection, tracking, measurement, etc) on object types not yet covered by the community data and pre-trained analysis pipelines available. This will involve training new models based on ground-truth annotations. Training configurations are listed on the pipeline page . How can I load data incrementally? If you have data in lots of places or it arrives at different times, it's probably best to break these batches or groups into individual datasets and annotate each individually. Using the checkboxes in web, you can use multiple datasets to generate a trained model. Breaking large amounts of data up into manageable groups is generally a good idea. Do users need to transcode their own data? No. VIAME Web and DIVE Desktop perform automatic transcoding if it is necessary. How does video frame alignment work? When you annotate a video in DIVE, the true video is played in the browser using a native HTML5 video player. Web browsers report and control time in floating point seconds rather than using frame numbers, but annotations are created using frame numbers as their time indicators, so it's important to make sure these line up. Most of the time, videos are downsampled for annotation, meaning that the true video framerate (30hz, for example) is annotated at a lower rate, such as 5hz or 10hz. Kwiver (the computer vision tool behind VIAME) uses a downsampling approach that sorts actual frames into downsampled buckets based on the start time of the frame. An implementation of this approach is described here. 1 2 3 4 5 6 7 8 9 10 def get_frame_from_timestamp ( timestamp , true_fps , downsample_fps ): downsampled_frame = timestamp * downsample_fps real_frame = if downsample_fps >= true_fps : # This is a true downsample next_true_frame_boundary = Math . ceil ( timestamp * true_fps ) return Math . floor ( next_true_frame_boundary / downsample_fps ) raise Exception ( 'Real video framerate must be GTE downsample rate' ) There are caveats with this approach. It does not handle padding properly. If a video begins or ends with padding, you may see a black screen in DIVE, but kwiver will wait for the first true frame to use as the representative for the bucket. It does not handle variable width frames properly. If a video has variable width frames, the assumptions about the locations of true frame boundaries do not hold and kwiver training may have alignment issues. Can I request new features or provide feedback? Yes! Please contact us at viame-web@kitware.com or log an issue directly on the issue tracker .","title":"Frequently Asked Questions"},{"location":"FAQ/#frequently-asked-questions","text":"","title":"Frequently Asked Questions"},{"location":"FAQ/#how-do-i-find-existing-data-to-use","text":"There are two places to look. The Training Data Collection is organized rougly by domain and collection method. The Stats Summary Page lists every published dataset organized by object label.","title":"How do I find existing data to use?"},{"location":"FAQ/#how-do-i-share-data-with-others","text":"This use case is covered on the sharing page . If you want to publish your data so that other groups can use it, please email viame-web@kitware.com .","title":"How do I share data with others?"},{"location":"FAQ/#how-do-i-run-analysis-workflows-on-my-data","text":"In DIVE, these are called pipelines. You'll need to see what sorts of analysis workflows are currently available on the pipeline page . These sorts of AI workflows are the final goal for most users. They allow the user to quickly perform quantitative analysis to answer questions like how many individuals of each type appear on each image or video frame? If no suitable existing analysis exists for your use case or you aren't sure how to proceed, you're welcome to email our team and ask for help .","title":"How do I run analysis workflows on my data?"},{"location":"FAQ/#how-do-i-create-new-models","text":"You want to perform analysis (detection, tracking, measurement, etc) on object types not yet covered by the community data and pre-trained analysis pipelines available. This will involve training new models based on ground-truth annotations. Training configurations are listed on the pipeline page .","title":"How do I create new models?"},{"location":"FAQ/#how-can-i-load-data-incrementally","text":"If you have data in lots of places or it arrives at different times, it's probably best to break these batches or groups into individual datasets and annotate each individually. Using the checkboxes in web, you can use multiple datasets to generate a trained model. Breaking large amounts of data up into manageable groups is generally a good idea.","title":"How can I load data incrementally?"},{"location":"FAQ/#do-users-need-to-transcode-their-own-data","text":"No. VIAME Web and DIVE Desktop perform automatic transcoding if it is necessary.","title":"Do users need to transcode their own data?"},{"location":"FAQ/#how-does-video-frame-alignment-work","text":"When you annotate a video in DIVE, the true video is played in the browser using a native HTML5 video player. Web browsers report and control time in floating point seconds rather than using frame numbers, but annotations are created using frame numbers as their time indicators, so it's important to make sure these line up. Most of the time, videos are downsampled for annotation, meaning that the true video framerate (30hz, for example) is annotated at a lower rate, such as 5hz or 10hz. Kwiver (the computer vision tool behind VIAME) uses a downsampling approach that sorts actual frames into downsampled buckets based on the start time of the frame. An implementation of this approach is described here. 1 2 3 4 5 6 7 8 9 10 def get_frame_from_timestamp ( timestamp , true_fps , downsample_fps ): downsampled_frame = timestamp * downsample_fps real_frame = if downsample_fps >= true_fps : # This is a true downsample next_true_frame_boundary = Math . ceil ( timestamp * true_fps ) return Math . floor ( next_true_frame_boundary / downsample_fps ) raise Exception ( 'Real video framerate must be GTE downsample rate' ) There are caveats with this approach. It does not handle padding properly. If a video begins or ends with padding, you may see a black screen in DIVE, but kwiver will wait for the first true frame to use as the representative for the bucket. It does not handle variable width frames properly. If a video has variable width frames, the assumptions about the locations of true frame boundaries do not hold and kwiver training may have alignment issues.","title":"How does video frame alignment work?"},{"location":"FAQ/#can-i-request-new-features-or-provide-feedback","text":"Yes! Please contact us at viame-web@kitware.com or log an issue directly on the issue tracker .","title":"Can I request new features or provide feedback?"},{"location":"Mouse-Keyboard-Shortcuts/","text":"Mouse and Keyboard Shortcuts General Interactions control description Left Click select track/detection Right Click toggle edit mode Middle Click pan camera Scroll Wheel zoom Mouse Drag pan shift + mouse drag select area to zoom Up Arrow select previous track in list Down Arrow select next track in list escape unselect, exit edit mode a toggle attribute / merge pane Playback control description Left Arrow or d previous frame Right Arrow or f next frame space play/pause Editing Most editing controls are available when a track or detection is selected. control description delete delete entire track or detection n create new track or detection Home or End go to first or last frame of selected 1 Enter bounding-box edit mode on selection 2 Enter polygon edit mode on selection 3 Enter head/tail/line edit mode on selection h or g while in line mode, place head point next t or y while in line mode, place tail point next escape unselect, exit edit mode, exit merge mode k toggle keyframe for the current frame and selected track i toggle interpolation for the current range of the selected track m enter merge mode for on selection shift + m commit (finalize) merge for selected tracks. shift + enter focus class select/text box on selected track in track list. Press Down Arrow to open all options. Pres enter twice to accept an option. Press escape to unfocus. Adding new shortcuts If you think a new shortcut would be useful, please send us feedback .","title":"Mouse and Keyboard Shortcuts"},{"location":"Mouse-Keyboard-Shortcuts/#mouse-and-keyboard-shortcuts","text":"","title":"Mouse and Keyboard Shortcuts"},{"location":"Mouse-Keyboard-Shortcuts/#general-interactions","text":"control description Left Click select track/detection Right Click toggle edit mode Middle Click pan camera Scroll Wheel zoom Mouse Drag pan shift + mouse drag select area to zoom Up Arrow select previous track in list Down Arrow select next track in list escape unselect, exit edit mode a toggle attribute / merge pane","title":"General Interactions"},{"location":"Mouse-Keyboard-Shortcuts/#playback","text":"control description Left Arrow or d previous frame Right Arrow or f next frame space play/pause","title":"Playback"},{"location":"Mouse-Keyboard-Shortcuts/#editing","text":"Most editing controls are available when a track or detection is selected. control description delete delete entire track or detection n create new track or detection Home or End go to first or last frame of selected 1 Enter bounding-box edit mode on selection 2 Enter polygon edit mode on selection 3 Enter head/tail/line edit mode on selection h or g while in line mode, place head point next t or y while in line mode, place tail point next escape unselect, exit edit mode, exit merge mode k toggle keyframe for the current frame and selected track i toggle interpolation for the current range of the selected track m enter merge mode for on selection shift + m commit (finalize) merge for selected tracks. shift + enter focus class select/text box on selected track in track list. Press Down Arrow to open all options. Pres enter twice to accept an option. Press escape to unfocus.","title":"Editing"},{"location":"Mouse-Keyboard-Shortcuts/#adding-new-shortcuts","text":"If you think a new shortcut would be useful, please send us feedback .","title":"Adding new shortcuts"},{"location":"Pipeline-Documentation/","text":"Pipelines and Training Both web and desktop versions are capable of running canned pipelines and model training on your ground truth data. This document is to help you decide which pipeline to run. Help me choose Contact our team if you need help choosing the right data analysis strategy. Please upload some sample data to viame.kitware.com to allow us to better assist you. Detection Best for a series of images that have no temporal relationship, such as arial photography of multiple scenes. Also preferred if you only care about aggregate date for the dataset, such as max occurrences of an object per scene. Pipeline Use case arctic seal eo yolo detector for color imagery arctic seal ir yolo detector for infrared em tuna detector for identifying individual features of tuna fish without motion simple single-class fish detector generic proposals generic object detector motion detect regions of motion in video or time-series images pengcam kw Penguin cam full-frame classifier pengcam swfsc Penguin cam full-frame classifier scallop and flatfish detector for benthic images scallop and flatfish left detector for benthic images, process left half of each frame only scallop netharn deep learning detector for benthic images scallop netharn left deep learning detector for benthic images, process left half of each frame only sea lion multi class detects bulls, cows, pups, etc sea lion single class detector sefsc bw group black-and-white fish detector (18 class, lower granularity) (oldest, v1) sefsc bw species v2 black-and-white fish species detector (updated, v2) Tracking Run full tracking pipelines on your data. Appropriate for videos and image sequences that derive from a video. Tracking involves first running a detection pipeline then performing detection linking to form connected object tracks. Note some trackers can perform differently on time-series data depending on the annotation framerate selected when you upload or import your dataset. Higer framerates take longer to process, but may produce better results. Pipeline Use case em tuna tracker fish simple fish tracker fish.sfd tracker generic generic object tracker puts generic boxs around arbitrary objects motion identifies moving object tracks mouss tracker, trained with data from MOUSS (Modular Optical Underwater Survey System) sefsc bw * same as above, but with tracking Utility An assortment of other types of utility pipelines. Utility piplines are named utility_<name>.pipe and are unique in that they may take detections as inputs (but are not required to). Pipeline Use case add segmentations watershed Transform existing bounding boxes into polygons empty frame lbls {N}fr Add an empty bounding box covering the whole media element for the purpose of adding full-frame classifier attributes. Unique tracks are created every N frames. track user selections Create tracks from user-initialized detection bounding boxes. Draw a box on the first frame of a track, and the pipeline will continue tracking the selected object(s) Training Run model training on ground truth annotations. Currently, training configurations are available to do object detection, object classification, and full-frame classification. Tracker training will be added in a future update. Full-frame classifiers can be trained on arbitrary multi-class labels. It's helpful to start with empty frame lbls utility pipe and add type annotations to each generated frame. Object classifiers and detectors are trained on bounding boxes with arbitrary multi-class labels. Overview SVM ( Support Vector Machine ) configurations are usable with the smallest amount of ground-truth and train relatively quickly. NetHarn is a pytorch deep learning framework that requires more input data: on the order of thousands of target examples. There are two architectures used. Netharn models can take up to several days to train. Cascade Faster R-CNN (cfrnn) for training box detectors Mask R-CNN for training pixel classification and box detection ResNet (Residual Network) for training full frame or secondary object classifiers Options New Model Name A recognizable name for the pipeline that results from the training run. Configuration File One of the configuration options in the table below. Labels.txt file This optional file controls the output classes that a newly trained model will generate. Use if you annotated using higher granularity labels (such as species names) and want to train a classifier using more Or you want to restrict your training session to only train on certain kinds of ground-truth data. The following example labels.txt shows how to train a FISH classifier by combining redfish and bluefish , preserve the ROCK label, and omit every other label. 1 2 FISH redfish bluefish ROCK By default, all classes from all input datasets are preserved in the output model. Use annotation frames only By default, training runs include all frames from the chosen input datasets, and frames without annotations are considered negatives examples. If you choose to use annotated frames only, frames or images with zero annotations will be discarded. This option is useful for trying to train on datasets that are only partially annotated. Configurations Configuration Availability Use Case detector_default both alias: train detector netharn cfrnn detector_netharn_cfrnn both detector_netharn_mask_rcnn both detector_svm_over_generic_detections both general purpose svm detector_svm_over_fish_detections both fish svm frame_classifier_default both alias: frame classifier netharn resnet frame_classifier_netharn_resnet both full-frame frame_classifier_svm_overn_resnet both full-frame object_classifier_default both alias: netharn resnet object classifier object_classifier_netharn_resnet both yolo desktop only can train, but resulting model is not runnable with desktop yet Custom Pipelines Pipelines created outside of VIAME Web can be upload and shared with other users. See Custom Pipeline Upload for details.","title":"Pipelines & Training"},{"location":"Pipeline-Documentation/#pipelines-and-training","text":"Both web and desktop versions are capable of running canned pipelines and model training on your ground truth data. This document is to help you decide which pipeline to run.","title":"Pipelines and Training"},{"location":"Pipeline-Documentation/#help-me-choose","text":"Contact our team if you need help choosing the right data analysis strategy. Please upload some sample data to viame.kitware.com to allow us to better assist you.","title":"Help me choose"},{"location":"Pipeline-Documentation/#detection","text":"Best for a series of images that have no temporal relationship, such as arial photography of multiple scenes. Also preferred if you only care about aggregate date for the dataset, such as max occurrences of an object per scene. Pipeline Use case arctic seal eo yolo detector for color imagery arctic seal ir yolo detector for infrared em tuna detector for identifying individual features of tuna fish without motion simple single-class fish detector generic proposals generic object detector motion detect regions of motion in video or time-series images pengcam kw Penguin cam full-frame classifier pengcam swfsc Penguin cam full-frame classifier scallop and flatfish detector for benthic images scallop and flatfish left detector for benthic images, process left half of each frame only scallop netharn deep learning detector for benthic images scallop netharn left deep learning detector for benthic images, process left half of each frame only sea lion multi class detects bulls, cows, pups, etc sea lion single class detector sefsc bw group black-and-white fish detector (18 class, lower granularity) (oldest, v1) sefsc bw species v2 black-and-white fish species detector (updated, v2)","title":"Detection"},{"location":"Pipeline-Documentation/#tracking","text":"Run full tracking pipelines on your data. Appropriate for videos and image sequences that derive from a video. Tracking involves first running a detection pipeline then performing detection linking to form connected object tracks. Note some trackers can perform differently on time-series data depending on the annotation framerate selected when you upload or import your dataset. Higer framerates take longer to process, but may produce better results. Pipeline Use case em tuna tracker fish simple fish tracker fish.sfd tracker generic generic object tracker puts generic boxs around arbitrary objects motion identifies moving object tracks mouss tracker, trained with data from MOUSS (Modular Optical Underwater Survey System) sefsc bw * same as above, but with tracking","title":"Tracking"},{"location":"Pipeline-Documentation/#utility","text":"An assortment of other types of utility pipelines. Utility piplines are named utility_<name>.pipe and are unique in that they may take detections as inputs (but are not required to). Pipeline Use case add segmentations watershed Transform existing bounding boxes into polygons empty frame lbls {N}fr Add an empty bounding box covering the whole media element for the purpose of adding full-frame classifier attributes. Unique tracks are created every N frames. track user selections Create tracks from user-initialized detection bounding boxes. Draw a box on the first frame of a track, and the pipeline will continue tracking the selected object(s)","title":"Utility"},{"location":"Pipeline-Documentation/#training","text":"Run model training on ground truth annotations. Currently, training configurations are available to do object detection, object classification, and full-frame classification. Tracker training will be added in a future update. Full-frame classifiers can be trained on arbitrary multi-class labels. It's helpful to start with empty frame lbls utility pipe and add type annotations to each generated frame. Object classifiers and detectors are trained on bounding boxes with arbitrary multi-class labels.","title":"Training"},{"location":"Pipeline-Documentation/#overview","text":"SVM ( Support Vector Machine ) configurations are usable with the smallest amount of ground-truth and train relatively quickly. NetHarn is a pytorch deep learning framework that requires more input data: on the order of thousands of target examples. There are two architectures used. Netharn models can take up to several days to train. Cascade Faster R-CNN (cfrnn) for training box detectors Mask R-CNN for training pixel classification and box detection ResNet (Residual Network) for training full frame or secondary object classifiers","title":"Overview"},{"location":"Pipeline-Documentation/#options","text":"","title":"Options"},{"location":"Pipeline-Documentation/#new-model-name","text":"A recognizable name for the pipeline that results from the training run.","title":"New Model Name"},{"location":"Pipeline-Documentation/#configuration-file","text":"One of the configuration options in the table below.","title":"Configuration File"},{"location":"Pipeline-Documentation/#labelstxt-file","text":"This optional file controls the output classes that a newly trained model will generate. Use if you annotated using higher granularity labels (such as species names) and want to train a classifier using more Or you want to restrict your training session to only train on certain kinds of ground-truth data. The following example labels.txt shows how to train a FISH classifier by combining redfish and bluefish , preserve the ROCK label, and omit every other label. 1 2 FISH redfish bluefish ROCK By default, all classes from all input datasets are preserved in the output model.","title":"Labels.txt file"},{"location":"Pipeline-Documentation/#use-annotation-frames-only","text":"By default, training runs include all frames from the chosen input datasets, and frames without annotations are considered negatives examples. If you choose to use annotated frames only, frames or images with zero annotations will be discarded. This option is useful for trying to train on datasets that are only partially annotated.","title":"Use annotation frames only"},{"location":"Pipeline-Documentation/#configurations","text":"Configuration Availability Use Case detector_default both alias: train detector netharn cfrnn detector_netharn_cfrnn both detector_netharn_mask_rcnn both detector_svm_over_generic_detections both general purpose svm detector_svm_over_fish_detections both fish svm frame_classifier_default both alias: frame classifier netharn resnet frame_classifier_netharn_resnet both full-frame frame_classifier_svm_overn_resnet both full-frame object_classifier_default both alias: netharn resnet object classifier object_classifier_netharn_resnet both yolo desktop only can train, but resulting model is not runnable with desktop yet","title":"Configurations"},{"location":"Pipeline-Documentation/#custom-pipelines","text":"Pipelines created outside of VIAME Web can be upload and shared with other users. See Custom Pipeline Upload for details.","title":"Custom Pipelines"},{"location":"Pipeline-Upload/","text":"Custom Pipeline Upload It's possible to upload custom pipes to DIVE Web through the girder interface. Warning This feature is not yet standardized, and the instructions below may change. Open the girder interface at /girder and create a new private folder called MyPipelines For our demo instance, open https://viame.kitware.com/girder Create a new folder in that private folder, and give it a name you'd like to associate with your new pipeline. Upload one or more files inside your new pipeline subfolder: A pipeline file ending in the .pipe file extension Whatever other model .zip files are required by the pipe, named exactly as they appear in your .pipe file above. Finally, set the pipeline folder metadata key trained_pipeline with value true . Your new pipeline will be available under the Run Pipeline -> Trained menu from the DIVE web app. Accepting input If your pipe must accept input, set the pipeline folder metadata property requires_input to true . Including base pipelines User-uploaded pipelines may depend on any pipe already installed from the base image or an addon using include <pipename>.pipe . Depending on other user-uploaded pipes is not supported. Tip KWIVER pipe files can be exported for use with DIVE using kwiver pipe-config","title":"Custom Pipeline Upload"},{"location":"Pipeline-Upload/#custom-pipeline-upload","text":"It's possible to upload custom pipes to DIVE Web through the girder interface. Warning This feature is not yet standardized, and the instructions below may change. Open the girder interface at /girder and create a new private folder called MyPipelines For our demo instance, open https://viame.kitware.com/girder Create a new folder in that private folder, and give it a name you'd like to associate with your new pipeline. Upload one or more files inside your new pipeline subfolder: A pipeline file ending in the .pipe file extension Whatever other model .zip files are required by the pipe, named exactly as they appear in your .pipe file above. Finally, set the pipeline folder metadata key trained_pipeline with value true . Your new pipeline will be available under the Run Pipeline -> Trained menu from the DIVE web app.","title":"Custom Pipeline Upload"},{"location":"Pipeline-Upload/#accepting-input","text":"If your pipe must accept input, set the pipeline folder metadata property requires_input to true .","title":"Accepting input"},{"location":"Pipeline-Upload/#including-base-pipelines","text":"User-uploaded pipelines may depend on any pipe already installed from the base image or an addon using include <pipename>.pipe . Depending on other user-uploaded pipes is not supported. Tip KWIVER pipe files can be exported for use with DIVE using kwiver pipe-config","title":"Including base pipelines"},{"location":"Screenshots/","text":"Screenshots This page provides a general overview of the differences between desktop and web through screenshots. Browse files Web Desktop Jobs List Web Desktop Annotator Web Desktop Training Config Web Desktop Settings Desktop","title":"Screenshots"},{"location":"Screenshots/#screenshots","text":"This page provides a general overview of the differences between desktop and web through screenshots.","title":"Screenshots"},{"location":"Screenshots/#browse-files","text":"Web Desktop","title":"Browse files"},{"location":"Screenshots/#jobs-list","text":"Web Desktop","title":"Jobs List"},{"location":"Screenshots/#annotator","text":"Web Desktop","title":"Annotator"},{"location":"Screenshots/#training-config","text":"Web Desktop","title":"Training Config"},{"location":"Screenshots/#settings","text":"Desktop","title":"Settings"},{"location":"UI-Annotation-View/","text":"Annotation View The Annotation View is where images an the annotations reside. Annotations have different states and properties based on the current mode. Left Click - this will select an annotation and highlight cyan Right Click - select an annotation and place it in editing mode allowing to edit existing geometry or create new ones Middle Click - Pan the camera. This is useful in polygon editing mode or while zoomen and creating annotations. Default Mode - In the default mode the annotation will have bounds associated with it as well as a text name for the type and an associated confidence level. The color and styling will match what is specified in the TypeList Settings. There are additional modes which can be toggled on and off in the . Selected Annotation - selected annotations are cyan in color Editing Annotation - Editing annotations are cyan in color and provide handles to resize the annotation as well as a central handle to move the annotation to different spot. Creating Annotation - Creating an annotation requires clicking and dragging the mouse. When the AnnotionView is ready to create the annotation the mouse will turn into a crosshair. Interpolated Annotation - If an annotation has interpolation and the current frame isn't a keyframe it will appear slightly faded and will become a keyframe if the user edits the size or position","title":"Annotation View"},{"location":"UI-Annotation-View/#annotation-view","text":"The Annotation View is where images an the annotations reside. Annotations have different states and properties based on the current mode. Left Click - this will select an annotation and highlight cyan Right Click - select an annotation and place it in editing mode allowing to edit existing geometry or create new ones Middle Click - Pan the camera. This is useful in polygon editing mode or while zoomen and creating annotations. Default Mode - In the default mode the annotation will have bounds associated with it as well as a text name for the type and an associated confidence level. The color and styling will match what is specified in the TypeList Settings. There are additional modes which can be toggled on and off in the . Selected Annotation - selected annotations are cyan in color Editing Annotation - Editing annotations are cyan in color and provide handles to resize the annotation as well as a central handle to move the annotation to different spot. Creating Annotation - Creating an annotation requires clicking and dragging the mouse. When the AnnotionView is ready to create the annotation the mouse will turn into a crosshair. Interpolated Annotation - If an annotation has interpolation and the current frame isn't a keyframe it will appear slightly faded and will become a keyframe if the user edits the size or position","title":"Annotation View"},{"location":"UI-Edit-Bar/","text":"View and Edit Controls The View/Edit Bar is used to hide/view annotations as well as create and edit them. Tthe visibility section where different types of geometry associated with a track/detection can be hidden or shown. The edit section allows for the creation and editing of different geometries associated with a track. Visibility - Toggle on/off the different geometries associated with tracks or detections. This may be necessary when having multiple geometries visbile to make it easier to see a specific type. By default all tracks/detections have an included bounding box. If there is a polygon as well as a bounding box the box will become thinner and dimmer. This tool allows you to disable one or the other to make groups of items more visible. Edit Section - This bar is used for both editing and creating geometries associated with a track. Edit/Creation Mode - When in edit or creation mode the selected geometry type will be highlighted blue. You can swap between them at anytime by clicking on the type or using the keyboard shortcuts corresponding to the numbers. If a geometry of that type already exists it will be placed in edit mode, if it doesn't exist you can then create it. Deletion - Allows for the deletion of geometry types or points within a geometry. Delete Geometry - This is the deletion of the entire type from the track. You cannot delete a bounding box. The bounding box is necessary for the representation of the track Delete Point - When in edit mode for a polygon or a line segment it is possible to select individual points by clicking on them. They will turn Red and the Deletion button will indicate that you are now selecting a point for deletion. Clicking it will remove that current point from the line segment or the polygon.","title":"Edit Bar"},{"location":"UI-Edit-Bar/#view-and-edit-controls","text":"The View/Edit Bar is used to hide/view annotations as well as create and edit them. Tthe visibility section where different types of geometry associated with a track/detection can be hidden or shown. The edit section allows for the creation and editing of different geometries associated with a track. Visibility - Toggle on/off the different geometries associated with tracks or detections. This may be necessary when having multiple geometries visbile to make it easier to see a specific type. By default all tracks/detections have an included bounding box. If there is a polygon as well as a bounding box the box will become thinner and dimmer. This tool allows you to disable one or the other to make groups of items more visible. Edit Section - This bar is used for both editing and creating geometries associated with a track. Edit/Creation Mode - When in edit or creation mode the selected geometry type will be highlighted blue. You can swap between them at anytime by clicking on the type or using the keyboard shortcuts corresponding to the numbers. If a geometry of that type already exists it will be placed in edit mode, if it doesn't exist you can then create it. Deletion - Allows for the deletion of geometry types or points within a geometry. Delete Geometry - This is the deletion of the entire type from the track. You cannot delete a bounding box. The bounding box is necessary for the representation of the track Delete Point - When in edit mode for a polygon or a line segment it is possible to select individual points by clicking on them. They will turn Red and the Deletion button will indicate that you are now selecting a point for deletion. Clicking it will remove that current point from the line segment or the polygon.","title":"View and Edit Controls"},{"location":"UI-Interpolation/","text":"Interpolation Interpolation allows the guessing of bounding boxes between sets of \"keyframes\". The intermediate bounding boxes are calculated between the keyframes and are adjusted accordingly. Interpolation allows a track to be created and edited much faster than drawing a detection for every single frame. See the Annotation Quickstart for a quick overview of enabling and using interpolation. Interpolation will be enabled on new tracks by default. If it isn't enabled it can be found under the \"+Track Settings\". Interpolation editing for existing tracks will only be enabled on tracks that span more than one frame. Basics Interpolation is done by creating a detection then moving the current frame forward in time and placing the detection in a new spot. Each time the detection is edited or moved it becomes a \"locked\" keyframe meaning it is used in the interpolation. Interpolated Frame that is being edited has no name associated with it and has a light highlight for the bounds: Track Interpolation Controls If the selected track spans more than one frame there will be interpolation controls available. Delete - Delete the entire track Split - Splits tracks that span more than one frame into two new tracks Star [KeyFrame] - Represents if the current frame is a keyframe or not. Filled in means it is a keyframe Interpolation Mode - The dashed rectangles mean that the current space without keyframes is interpolated. To show an occluded object this would be turned to off. First Frame - '<<' Previous Keyframe - '<' Next Keyframe - '> ' Last Frame - '>>' Edit Toggle - will toggle the currently selected track edit mode Event Viewer The event viewer provides a quick and concise view of an interpolated track. Keyframes - represented by solid individual markers in the track Interpolated Ranges - represented by a thin cyan line joining keyframes Blank Ranges - areas absent of keyframes and interpolated ranges. Interpolated Extensive Demo Below is a longer demo showing creation of interpolated tracks as well as converting some areas to Blank ranges which contain no annotations.","title":"Interpolation"},{"location":"UI-Interpolation/#interpolation","text":"Interpolation allows the guessing of bounding boxes between sets of \"keyframes\". The intermediate bounding boxes are calculated between the keyframes and are adjusted accordingly. Interpolation allows a track to be created and edited much faster than drawing a detection for every single frame. See the Annotation Quickstart for a quick overview of enabling and using interpolation. Interpolation will be enabled on new tracks by default. If it isn't enabled it can be found under the \"+Track Settings\". Interpolation editing for existing tracks will only be enabled on tracks that span more than one frame.","title":"Interpolation"},{"location":"UI-Interpolation/#basics","text":"Interpolation is done by creating a detection then moving the current frame forward in time and placing the detection in a new spot. Each time the detection is edited or moved it becomes a \"locked\" keyframe meaning it is used in the interpolation. Interpolated Frame that is being edited has no name associated with it and has a light highlight for the bounds:","title":"Basics"},{"location":"UI-Interpolation/#track-interpolation-controls","text":"If the selected track spans more than one frame there will be interpolation controls available. Delete - Delete the entire track Split - Splits tracks that span more than one frame into two new tracks Star [KeyFrame] - Represents if the current frame is a keyframe or not. Filled in means it is a keyframe Interpolation Mode - The dashed rectangles mean that the current space without keyframes is interpolated. To show an occluded object this would be turned to off. First Frame - '<<' Previous Keyframe - '<' Next Keyframe - '> ' Last Frame - '>>' Edit Toggle - will toggle the currently selected track edit mode","title":"Track Interpolation Controls"},{"location":"UI-Interpolation/#event-viewer","text":"The event viewer provides a quick and concise view of an interpolated track. Keyframes - represented by solid individual markers in the track Interpolated Ranges - represented by a thin cyan line joining keyframes Blank Ranges - areas absent of keyframes and interpolated ranges.","title":"Event Viewer"},{"location":"UI-Interpolation/#interpolated-extensive-demo","text":"Below is a longer demo showing creation of interpolated tracks as well as converting some areas to Blank ranges which contain no annotations.","title":"Interpolated Extensive Demo"},{"location":"UI-Navigation-Bar/","text":"Navigation Bar Data Link - Returns back to the folder which contains the current data. Run Pipeline - Will run a pipeline from the dropdown on the current data. NOTE: Current annotations will be backed-up and replaced by the pipeline when it is complete Download - Allows for downloading all data, the image-sequence/video, dataset configuration, or just the detections. Exclude Tracks - this allows you to remove tracks below a specific confidence threshold when exporting the CSV. It is how you can export only the higher detections/tracks after running a pipeline. Checked Types Only - allows you to only export the annotations of types that are currently checked in the type list. Help - Provides a small indication of currently available mouse/keyboard shortcuts as well as a link to this documention. Save - This button is used to save the current annotations and any custom styles applied to the different types. Changes are not immediately committed and will instead update the save icon with a number indicating the number of changes that have occured. Clicking this button will reset the number and save the data at the same time.","title":"Navigation Bar"},{"location":"UI-Navigation-Bar/#navigation-bar","text":"Data Link - Returns back to the folder which contains the current data. Run Pipeline - Will run a pipeline from the dropdown on the current data. NOTE: Current annotations will be backed-up and replaced by the pipeline when it is complete Download - Allows for downloading all data, the image-sequence/video, dataset configuration, or just the detections. Exclude Tracks - this allows you to remove tracks below a specific confidence threshold when exporting the CSV. It is how you can export only the higher detections/tracks after running a pipeline. Checked Types Only - allows you to only export the annotations of types that are currently checked in the type list. Help - Provides a small indication of currently available mouse/keyboard shortcuts as well as a link to this documention. Save - This button is used to save the current annotations and any custom styles applied to the different types. Changes are not immediately committed and will instead update the save icon with a number indicating the number of changes that have occured. Clicking this button will reset the number and save the data at the same time.","title":"Navigation Bar"},{"location":"UI-Timeline/","text":"TimeLine The Timeline view provides an ability to quickly see data across the length of the video/image-sequence. Above it are standard video controls for controlling playback. Additionally a \"Left Click\" anywhere within the timeline will automatically seek to that frame. The current frame in the timeline is represented by a vertical cyan bar. Switching between the different modes can be done by clicking on the text either for Detections or Events . The Arrow on the left side of the timeline view can be used to minimize the view to provide more screen space for annotations. On the right side of the Timeline is a button used to recenter the camera on the annotation. Event View The default event viewer shows a representation of the start/stop frames for the tracks filtered by the TypeList. The Tracks are presented using their corresponding type colors. When hovering over any Track the TrackID will display. Clicking on that track will select the track and transition the current frame to that frame. Single Frame Detections - Single frame detections are presented as single frames with spaces between. Selected Track View - A selected track will be cyan and will cause the other tracks to fade out. Selected Interpolated Track - A selected track which has keyframes and interpolation will show the areas of interpolation, the keyframes and the track. Detection Count This provides a count of the types over the duration of the track. This is updated in realtime with the confidence slider so it can be used to filter out higher densities of types as well as get an indication of the number of the types visible at any one time.","title":"TimeLine"},{"location":"UI-Timeline/#timeline","text":"The Timeline view provides an ability to quickly see data across the length of the video/image-sequence. Above it are standard video controls for controlling playback. Additionally a \"Left Click\" anywhere within the timeline will automatically seek to that frame. The current frame in the timeline is represented by a vertical cyan bar. Switching between the different modes can be done by clicking on the text either for Detections or Events . The Arrow on the left side of the timeline view can be used to minimize the view to provide more screen space for annotations. On the right side of the Timeline is a button used to recenter the camera on the annotation.","title":"TimeLine"},{"location":"UI-Timeline/#event-view","text":"The default event viewer shows a representation of the start/stop frames for the tracks filtered by the TypeList. The Tracks are presented using their corresponding type colors. When hovering over any Track the TrackID will display. Clicking on that track will select the track and transition the current frame to that frame. Single Frame Detections - Single frame detections are presented as single frames with spaces between. Selected Track View - A selected track will be cyan and will cause the other tracks to fade out. Selected Interpolated Track - A selected track which has keyframes and interpolation will show the areas of interpolation, the keyframes and the track.","title":"Event View"},{"location":"UI-Timeline/#detection-count","text":"This provides a count of the types over the duration of the track. This is updated in realtime with the confidence slider so it can be used to filter out higher densities of types as well as get an indication of the number of the types visible at any one time.","title":"Detection Count"},{"location":"UI-Track-List/","text":"Track List The tracklist allows for selecting and editing tracks that currently not filtered by the TypeList. The tracklist will present two different options depending on the Track that is selected. Single Detection - A track that spans a single frame. Delete Detection - Delete the detection Go to Frame - Goes to the first frame of the detection Toggle Edit Mode - selects the track and toggles edit mode on Multiple Frame Track - A track that spans multiple frames and has more options Delete Track - Delete the entire Track Split Track - Splits the track on the current Frame Toggle Keyframe - Star is filled in if the current frame annotation is a keyfame. Clicking this will either remove the keyframe if it exists or make the current interpolated annotation a keyframe. Toggle Interpolation - Turns interpolation on/off for a range between keyframes. Navigation Options - Navigation for first frame of track, then to jump between keyframes, finally the last frame of the track Toggle Edit Mode - selects the track and toggles edit mode on","title":"Track List"},{"location":"UI-Track-List/#track-list","text":"The tracklist allows for selecting and editing tracks that currently not filtered by the TypeList. The tracklist will present two different options depending on the Track that is selected. Single Detection - A track that spans a single frame. Delete Detection - Delete the detection Go to Frame - Goes to the first frame of the detection Toggle Edit Mode - selects the track and toggles edit mode on Multiple Frame Track - A track that spans multiple frames and has more options Delete Track - Delete the entire Track Split Track - Splits the track on the current Frame Toggle Keyframe - Star is filled in if the current frame annotation is a keyfame. Clicking this will either remove the keyframe if it exists or make the current interpolated annotation a keyframe. Toggle Interpolation - Turns interpolation on/off for a range between keyframes. Navigation Options - Navigation for first frame of track, then to jump between keyframes, finally the last frame of the track Toggle Edit Mode - selects the track and toggles edit mode on","title":"Track List"},{"location":"UI-Type-List/","text":"Type List The TypeList is used to control visual styles of the different types as well as filter out types that don't need to be displayed. The checkbox next to each type name can be used to toggle types on and off. Confidence Filter - for pipeline results the confidence filter can be used to filter out different items based on their current confidence level. As the confidence filter is adjusted the annotations in the AnnotationView as well as the Timeline views will update in realtime. Type Style Editor When hovering over a type there is an Edit icon that is visible. Clicking on that will bring up the TypeList Editor. The type editor allows you to change the visual representation of the Type within the annotation view as well as the color and name. Name - You can change the name for the type and it will update all subsequent tracks that are using that Type. Border Thickness - the line thickness can be changed to make a type stand out more or less Fill - Fill allows the bounding box to be filled. This is useful for annotation of background items in an image. Opacity - The opacity of the lines and the fill can be set here. Color - The color for the type within the annotations and the timeline views. Type (Class) Settings Each dataset maintains its own list of types (classes). These are customizable under type settings. Type settings are available by clicking the cog button next to Visiblity in the type filter list heading. Type Settings Options Ad-hoc mode In ad-hoc mode, new object classes are added as you annotate. The type list updates automatically when new classes are added or the last member of a class is deleted. Selecting Lock Types = false toggles ad-hoc mode. Selecting Show empty is recommended Locked mode In locked mode, only a specified list of classes can be used, and must be selected or autocompleted from the list for each object. Selecting Lock Types = true toggles locked mode. You can add types using the + Types button under type settings","title":"Type List"},{"location":"UI-Type-List/#type-list","text":"The TypeList is used to control visual styles of the different types as well as filter out types that don't need to be displayed. The checkbox next to each type name can be used to toggle types on and off. Confidence Filter - for pipeline results the confidence filter can be used to filter out different items based on their current confidence level. As the confidence filter is adjusted the annotations in the AnnotationView as well as the Timeline views will update in realtime.","title":"Type List"},{"location":"UI-Type-List/#type-style-editor","text":"When hovering over a type there is an Edit icon that is visible. Clicking on that will bring up the TypeList Editor. The type editor allows you to change the visual representation of the Type within the annotation view as well as the color and name. Name - You can change the name for the type and it will update all subsequent tracks that are using that Type. Border Thickness - the line thickness can be changed to make a type stand out more or less Fill - Fill allows the bounding box to be filled. This is useful for annotation of background items in an image. Opacity - The opacity of the lines and the fill can be set here. Color - The color for the type within the annotations and the timeline views.","title":"Type Style Editor"},{"location":"UI-Type-List/#type-class-settings","text":"Each dataset maintains its own list of types (classes). These are customizable under type settings. Type settings are available by clicking the cog button next to Visiblity in the type filter list heading.","title":"Type (Class) Settings"},{"location":"UI-Type-List/#type-settings-options","text":"","title":"Type Settings Options"},{"location":"UI-Type-List/#ad-hoc-mode","text":"In ad-hoc mode, new object classes are added as you annotate. The type list updates automatically when new classes are added or the last member of a class is deleted. Selecting Lock Types = false toggles ad-hoc mode. Selecting Show empty is recommended","title":"Ad-hoc mode"},{"location":"UI-Type-List/#locked-mode","text":"In locked mode, only a specified list of classes can be used, and must be selected or autocompleted from the list for each object. Selecting Lock Types = true toggles locked mode. You can add types using the + Types button under type settings","title":"Locked mode"},{"location":"User-Documentation/","text":"User Documentation Current capabilities of DIVE include: User import of frame images or video. Playback of existing annotation data. Manual creation of new annotations. Automatic object detection and tracking of user-imported data. Manual user refinement of automatically generated tracks Export of generated annotations. Controls control description f or right arrow skip forward 1 frame d or left arrow skip backward 1 frame h toggle annotate head position mode t toggle annotate tail position mode space play/pause mouse scroll zoom mouse drag pan shift + drag select area to zoom left-click select/de-select track right-click toggle track edit mode Register for an account A user account is required to store data and run pipelines on viame.kitware.com. Visit https://viame.kitware.com Click Register Adding a data source Open the DIVE Homepage, and navigate to the \"Data\" tab. Click the blue \"user home\" button at the top left of the data browser. Choose your \"public\" or \"private\" folder. Click the blue \"Upload\" button that appears in the toolbar. Select either an .mp4 video or multi-select a group of image frames. If you already have annotations.csv , select that too. Choose a name for the data, enter the optional video playback frame rate, and press start-upload. In the data browser, a new blue \"Annotate\" button will appear next to your data. General annotation controls General Definitions Track : a series of bounding box detections of a single object over time Detection a single bounding box on an individual frame Types : a set of mutually exclusive object classifications that can be applied to a Track, such as \"species\" if annotating animals. Meatballs menu : a horizontal 3-dot icon that expands to show a menu. General controls Select detection : left-click on a detection. Edit existing detection : right-click on a detection, adjust its position, then right click again to accept your changes. You'll notice the outline of the old detection go away when you successfully save the new one. Delete single detection : Press q when a detection is selected. Delete single track : In the track list, a meatballs menu will appear on hover. Click the meatballs menu and click \"Delete Track\". Creating a new track In the track list header, click the + button. Your cursor will turn into a cross-hair. Click-and-drag to draw a box over the object you intend to track. This creates a detection. If you edit it again, you must save the edits (see above). Use the f and d keys or the left/right arrows to advance forward and backward in time frame-by-frame. Notice that when you change the current frame, your cursor becomes a cross-hair again. left-click within the editable region of the final detection to complete track creation. left-click the track again to select it in the Track list. Double-click its type to open the list of available types, and select one. Adding head and tail point annotations When annotating fish, it's possible to add additional detection-level annotations for head and tail position. It's best to first create a complete track of a fish before adding head/tail annotations. Select the detection you wish to add head/tail points for by left-clicking it. To begin with the head, tap the h (or g ) key. Your cursor will change to a cross-hair. Left-click to place a marker. If the tail does not exist, your cursor will stay a cross-hair, and you can left-click the tail immediately to place another marker. Notes: You can also start with the tail: tap t (or y ) You don't have to place both markers. Press any of g , h , t , or y to exit head/tail annotation mode. Your cursor will change back to a pointer. You can modify an existing head/tail marker by tapping the corresponding key. If you place one marker and the other already exists, you will not be automatically prompted to place the second. To Delete a head/tail pair , select a detection with existing markers and tap q . When a track is selected, a meatball menu will appear in the top-right corner of the annotation window. All hotkey controls are also available on that menu. Persisting changes Annotations are not automatically persisted to the VIAME server. To save your changes, click the save disk at the right side of the top toolbar.","title":"User Documentation"},{"location":"User-Documentation/#user-documentation","text":"Current capabilities of DIVE include: User import of frame images or video. Playback of existing annotation data. Manual creation of new annotations. Automatic object detection and tracking of user-imported data. Manual user refinement of automatically generated tracks Export of generated annotations.","title":"User Documentation"},{"location":"User-Documentation/#controls","text":"control description f or right arrow skip forward 1 frame d or left arrow skip backward 1 frame h toggle annotate head position mode t toggle annotate tail position mode space play/pause mouse scroll zoom mouse drag pan shift + drag select area to zoom left-click select/de-select track right-click toggle track edit mode","title":"Controls"},{"location":"User-Documentation/#register-for-an-account","text":"A user account is required to store data and run pipelines on viame.kitware.com. Visit https://viame.kitware.com Click Register","title":"Register for an account"},{"location":"User-Documentation/#adding-a-data-source","text":"Open the DIVE Homepage, and navigate to the \"Data\" tab. Click the blue \"user home\" button at the top left of the data browser. Choose your \"public\" or \"private\" folder. Click the blue \"Upload\" button that appears in the toolbar. Select either an .mp4 video or multi-select a group of image frames. If you already have annotations.csv , select that too. Choose a name for the data, enter the optional video playback frame rate, and press start-upload. In the data browser, a new blue \"Annotate\" button will appear next to your data.","title":"Adding a data source"},{"location":"User-Documentation/#general-annotation-controls","text":"General Definitions Track : a series of bounding box detections of a single object over time Detection a single bounding box on an individual frame Types : a set of mutually exclusive object classifications that can be applied to a Track, such as \"species\" if annotating animals. Meatballs menu : a horizontal 3-dot icon that expands to show a menu. General controls Select detection : left-click on a detection. Edit existing detection : right-click on a detection, adjust its position, then right click again to accept your changes. You'll notice the outline of the old detection go away when you successfully save the new one. Delete single detection : Press q when a detection is selected. Delete single track : In the track list, a meatballs menu will appear on hover. Click the meatballs menu and click \"Delete Track\".","title":"General annotation controls"},{"location":"User-Documentation/#creating-a-new-track","text":"In the track list header, click the + button. Your cursor will turn into a cross-hair. Click-and-drag to draw a box over the object you intend to track. This creates a detection. If you edit it again, you must save the edits (see above). Use the f and d keys or the left/right arrows to advance forward and backward in time frame-by-frame. Notice that when you change the current frame, your cursor becomes a cross-hair again. left-click within the editable region of the final detection to complete track creation. left-click the track again to select it in the Track list. Double-click its type to open the list of available types, and select one.","title":"Creating a new track"},{"location":"User-Documentation/#adding-head-and-tail-point-annotations","text":"When annotating fish, it's possible to add additional detection-level annotations for head and tail position. It's best to first create a complete track of a fish before adding head/tail annotations. Select the detection you wish to add head/tail points for by left-clicking it. To begin with the head, tap the h (or g ) key. Your cursor will change to a cross-hair. Left-click to place a marker. If the tail does not exist, your cursor will stay a cross-hair, and you can left-click the tail immediately to place another marker. Notes: You can also start with the tail: tap t (or y ) You don't have to place both markers. Press any of g , h , t , or y to exit head/tail annotation mode. Your cursor will change back to a pointer. You can modify an existing head/tail marker by tapping the corresponding key. If you place one marker and the other already exists, you will not be automatically prompted to place the second. To Delete a head/tail pair , select a detection with existing markers and tap q . When a track is selected, a meatball menu will appear in the top-right corner of the annotation window. All hotkey controls are also available on that menu.","title":"Adding head and tail point annotations"},{"location":"User-Documentation/#persisting-changes","text":"Annotations are not automatically persisted to the VIAME server. To save your changes, click the save disk at the right side of the top toolbar.","title":"Persisting changes"},{"location":"Web-Version/","text":"Web Version Try our public server \u27a5 You can also run your own . Warning VIAME Web is automatically updated at 2AM EST/EDT every Thursday. If you are running a pipeline or training workflow during update, it will be interrupted and started over. Features upload and download data permissions and sharing support for team collaboration image and video annotation pre-trained model pipeline execution multi-dataset model training Register for an account A user account is required to store data and run pipelines on viame.kitware.com. Visit viame.kitware.com Click Register Uploading data Open the DIVE Homepage, and navigate to the \"Data\" tab. Click the blue \"user home\" button at the top left of the data browser. Choose your \"public\" or \"private\" folder. Click the blue \"Upload\" button that appears in the toolbar. Select a video or multi-select a group of image frames. If you already have annotations.csv , select that too. Choose a name for the data, enter the optional video playback frame rate, and press start-upload. In the data browser, a new blue \"Launch Annotator\" button will appear next to your data. Zip Files A zip import can have one of the following file combinations: One or more images, an optional annotation file, and an optional configuration file One video with an optional annotation file and an optional configuration file One or more folders which contain the above examples (These will be converted to separate datasets) Zip import also accepts zip archive files that were generated by DIVE Web's Download -> Everything export button. Info All video uploaded to the web server will be transcoded as mp4/h264 . Downloading Data After creating/modifying annotations or running pipelines the data may be exported through two methods: File Browser Data Download Annotation Viewer Download Annotation Folder Structure An annotation folder in DIVE will consist of three main components: Source Data - The image-sequence files or video used in the annotation. Detections File - The current detections/tracks for the annotation which are stored in either a CSV format or JSON After the first edit to a project, the data will be stored in JSON, when downloading the current detection it will always be downloaded in .csv format Auxiliary Folder - Backups of the detection file each time a save is done. File Browser Data Download Data can be downloaded directly from the FileBrowser by clicking the checkmark to the left of a folder. This allows you to download the source images/video, the current detection file converted to .csv or everything including all backups of the detection files. Filtered Detections - The ability to export detections/tracks based on the currently set confidence filter. This will only export tracks/detections that are higher than the confidence filter set for the folder. This can be adjusted through the UI interface within the Annotation Viewer. For more information check: Annotation Viewer Download Within the annotation viewer itself there is the option to download the same data in the file browser. These options are explained in Navigation Bar UI Docs. Trained model downloads You can download your trained models through the administrative interface. This will be added to the normal interface in a future update. Warning Use caution when modifying data through the admin interface Open the admin interface at https://viame.kitware.com/girder (or myserver.com/girder if you host your own instance) Navigate to your personal workspace by clicking My folders under your user dropdown in the top right corner. Navigate to the VIAME/VIAME Training Results folder and into the folder you wish to download Select all items and download using the menu Sharing data with teams This information will be relevant to teams where several people need to work on the same data. Concepts By default, data uploaded to your personal user space follows these conventions. Data in the Public folder is readable by all registered users, but writable only by you by default. Data in the Private folder is only visible to you by default. Note You can share your entire public or private folder with team members. Working with teams A common scenario is for a group to have a lot of shared data that several members should be able to view and annotate. For most teams, we recommend keeping data consolidated under a single account then following the sharing instructions below to make sure all team members have appropriate access. It's easiest to create a single parent directory to share and then put all individual datasets inside that parent. Warning You should note that 2 people cannot work on the same video at the same time. Your team should coordianate on who will work on each dataset. Managing Permissions DIVE uses Girder's Permissions Model . There are four levels of permission a User can have on a resource. No permission (cannot view, edit, or delete a resource) READ permission (can view and download resources) WRITE permission (includes READ permission, can edit the properties of a resource) ADMIN also known as own permission, (includes READ and WRITE permission, can delete the resource and also control access on it) Granting access to others Navigate to your data in the data browser. Right click a dataset folder or directory to share. Search for and select users you want to grant permissions for. Select the correct permissions in the drop-down next to each user. Be sure to enable Include subfolders at the bottom of the dialog. Click save. These users should now be able to view and edit your data. Data Shared with you You can view data shared with you by selecting the 'SHARED WITH ME' Tab above the data browser. Sharing URLs You can copy and paste any URL from the address bar and share with collaborators. This includes folders in the data browser as well as direct links to the annotation editor. Dataset Clones A clone is a shallow copy of a dataset. It has its own annotations, and can be run through pipelines and shared with others. It references the media (images or video) of another dataset. Clone use cases When you want to use or modify data that doesn't belong to you, such as data from the shared training collection or from other users. When you want to run several different pipelines in parallel on the same input data and compare the results. Warning Merging cloned data back to the source is not currently supported . To collaborate with others on annotations, the sharing use case above is preferred. How to clone Open the dataset you wish to clone in the viewer. Click the Clone chip in the top toolbar next to the name Choose a name and location for the clone within your own workspace.","title":"Web Version"},{"location":"Web-Version/#web-version","text":"Try our public server \u27a5 You can also run your own . Warning VIAME Web is automatically updated at 2AM EST/EDT every Thursday. If you are running a pipeline or training workflow during update, it will be interrupted and started over.","title":"Web Version"},{"location":"Web-Version/#features","text":"upload and download data permissions and sharing support for team collaboration image and video annotation pre-trained model pipeline execution multi-dataset model training","title":"Features"},{"location":"Web-Version/#register-for-an-account","text":"A user account is required to store data and run pipelines on viame.kitware.com. Visit viame.kitware.com Click Register","title":"Register for an account"},{"location":"Web-Version/#uploading-data","text":"Open the DIVE Homepage, and navigate to the \"Data\" tab. Click the blue \"user home\" button at the top left of the data browser. Choose your \"public\" or \"private\" folder. Click the blue \"Upload\" button that appears in the toolbar. Select a video or multi-select a group of image frames. If you already have annotations.csv , select that too. Choose a name for the data, enter the optional video playback frame rate, and press start-upload. In the data browser, a new blue \"Launch Annotator\" button will appear next to your data.","title":"Uploading data"},{"location":"Web-Version/#zip-files","text":"A zip import can have one of the following file combinations: One or more images, an optional annotation file, and an optional configuration file One video with an optional annotation file and an optional configuration file One or more folders which contain the above examples (These will be converted to separate datasets) Zip import also accepts zip archive files that were generated by DIVE Web's Download -> Everything export button. Info All video uploaded to the web server will be transcoded as mp4/h264 .","title":"Zip Files"},{"location":"Web-Version/#downloading-data","text":"After creating/modifying annotations or running pipelines the data may be exported through two methods: File Browser Data Download Annotation Viewer Download","title":"Downloading Data"},{"location":"Web-Version/#annotation-folder-structure","text":"An annotation folder in DIVE will consist of three main components: Source Data - The image-sequence files or video used in the annotation. Detections File - The current detections/tracks for the annotation which are stored in either a CSV format or JSON After the first edit to a project, the data will be stored in JSON, when downloading the current detection it will always be downloaded in .csv format Auxiliary Folder - Backups of the detection file each time a save is done.","title":"Annotation Folder Structure"},{"location":"Web-Version/#file-browser-data-download","text":"Data can be downloaded directly from the FileBrowser by clicking the checkmark to the left of a folder. This allows you to download the source images/video, the current detection file converted to .csv or everything including all backups of the detection files. Filtered Detections - The ability to export detections/tracks based on the currently set confidence filter. This will only export tracks/detections that are higher than the confidence filter set for the folder. This can be adjusted through the UI interface within the Annotation Viewer. For more information check:","title":"File Browser Data Download"},{"location":"Web-Version/#annotation-viewer-download","text":"Within the annotation viewer itself there is the option to download the same data in the file browser. These options are explained in Navigation Bar UI Docs.","title":"Annotation Viewer Download"},{"location":"Web-Version/#trained-model-downloads","text":"You can download your trained models through the administrative interface. This will be added to the normal interface in a future update. Warning Use caution when modifying data through the admin interface Open the admin interface at https://viame.kitware.com/girder (or myserver.com/girder if you host your own instance) Navigate to your personal workspace by clicking My folders under your user dropdown in the top right corner. Navigate to the VIAME/VIAME Training Results folder and into the folder you wish to download Select all items and download using the menu","title":"Trained model downloads"},{"location":"Web-Version/#sharing-data-with-teams","text":"This information will be relevant to teams where several people need to work on the same data.","title":"Sharing data with teams"},{"location":"Web-Version/#concepts","text":"By default, data uploaded to your personal user space follows these conventions. Data in the Public folder is readable by all registered users, but writable only by you by default. Data in the Private folder is only visible to you by default. Note You can share your entire public or private folder with team members.","title":"Concepts"},{"location":"Web-Version/#working-with-teams","text":"A common scenario is for a group to have a lot of shared data that several members should be able to view and annotate. For most teams, we recommend keeping data consolidated under a single account then following the sharing instructions below to make sure all team members have appropriate access. It's easiest to create a single parent directory to share and then put all individual datasets inside that parent. Warning You should note that 2 people cannot work on the same video at the same time. Your team should coordianate on who will work on each dataset.","title":"Working with teams"},{"location":"Web-Version/#managing-permissions","text":"DIVE uses Girder's Permissions Model . There are four levels of permission a User can have on a resource. No permission (cannot view, edit, or delete a resource) READ permission (can view and download resources) WRITE permission (includes READ permission, can edit the properties of a resource) ADMIN also known as own permission, (includes READ and WRITE permission, can delete the resource and also control access on it)","title":"Managing Permissions"},{"location":"Web-Version/#granting-access-to-others","text":"Navigate to your data in the data browser. Right click a dataset folder or directory to share. Search for and select users you want to grant permissions for. Select the correct permissions in the drop-down next to each user. Be sure to enable Include subfolders at the bottom of the dialog. Click save. These users should now be able to view and edit your data.","title":"Granting access to others"},{"location":"Web-Version/#data-shared-with-you","text":"You can view data shared with you by selecting the 'SHARED WITH ME' Tab above the data browser.","title":"Data Shared with you"},{"location":"Web-Version/#sharing-urls","text":"You can copy and paste any URL from the address bar and share with collaborators. This includes folders in the data browser as well as direct links to the annotation editor.","title":"Sharing URLs"},{"location":"Web-Version/#dataset-clones","text":"A clone is a shallow copy of a dataset. It has its own annotations, and can be run through pipelines and shared with others. It references the media (images or video) of another dataset.","title":"Dataset Clones"},{"location":"Web-Version/#clone-use-cases","text":"When you want to use or modify data that doesn't belong to you, such as data from the shared training collection or from other users. When you want to run several different pipelines in parallel on the same input data and compare the results. Warning Merging cloned data back to the source is not currently supported . To collaborate with others on annotations, the sharing use case above is preferred.","title":"Clone use cases"},{"location":"Web-Version/#how-to-clone","text":"Open the dataset you wish to clone in the viewer. Click the Clone chip in the top toolbar next to the name Choose a name and location for the clone within your own workspace.","title":"How to clone"}]}